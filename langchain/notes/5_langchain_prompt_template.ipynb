{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c90db2",
   "metadata": {},
   "source": [
    "# LangChain Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3e40d-9cff-432e-8624-c2e00bb33003",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46631614-c4d3-41e6-a949-b95dfd0ef4fc",
   "metadata": {},
   "source": [
    "**Types of Prompt**\n",
    "\n",
    "1. Static Prompt (Raw text sent by the used. example: \"What is the capital of India\")\n",
    "2. Dynamic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00068d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are a helpful assistant that answers strictly based on the provided context.\n",
      "\n",
      "        Guidelines:\n",
      "        - Use only the information in the Context to answer the Query.\n",
      "        - If the answer is not present in the Context, say \"I don't have enough information in the provided context to answer that.\"\n",
      "        - Be concise: 1 short paragraph, maximum 2-3 sentences.\n",
      "        - Do not invent facts, do not speculate, and do not use external knowledge.\n",
      "        - If multiple relevant points exist in Context, synthesize them clearly.\n",
      "        - Preserve any important terminology from the Context.\n",
      "\n",
      "        Context:\n",
      "        Virat Kohli is a Cricketer.\n",
      "\n",
      "        Query:\n",
      "        What sport does Virat play?\n",
      "\n",
      "        Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        You are a helpful assistant that answers strictly based on the provided context.\n",
    "\n",
    "        Guidelines:\n",
    "        - Use only the information in the Context to answer the Query.\n",
    "        - If the answer is not present in the Context, say \"I don't have enough information in the provided context to answer that.\"\n",
    "        - Be concise: 1 short paragraph, maximum 2-3 sentences.\n",
    "        - Do not invent facts, do not speculate, and do not use external knowledge.\n",
    "        - If multiple relevant points exist in Context, synthesize them clearly.\n",
    "        - Preserve any important terminology from the Context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Query:\n",
    "        {query}\n",
    "\n",
    "        Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"query\"],\n",
    ")\n",
    "\n",
    "# prompt: PromptValue\n",
    "prompt: PromptValue = prompt_template.invoke({\n",
    "    'context': 'Virat Kohli is a Cricketer.',\n",
    "    'query': 'What sport does Virat play?'\n",
    "})\n",
    "\n",
    "print(prompt.to_string())\n",
    "# print(prompt.to_messages()) # HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d3e90",
   "metadata": {},
   "source": [
    "### Why not a Python F-String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359a555-2e4d-4ed3-8f35-0508af3f3895",
   "metadata": {},
   "source": [
    "**We get a built-in validation, f-string does not provide that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f91447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Name:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello \n"
     ]
    }
   ],
   "source": [
    "name = input('Enter Your Name: ')\n",
    "print(f'Hello {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9730d-a5f0-47bc-a11c-6e3143c2b134",
   "metadata": {},
   "source": [
    "**Prompt Templates can be stored outside the codebase in a separate file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55223a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are a helpful assistant that answers strictly based on the provided context.\n",
      "\n",
      "        Guidelines:\n",
      "        - Use only the information in the Context to answer the Query.\n",
      "        - If the answer is not present in the Context, say \"I don't have enough information in the provided context to answer that.\"\n",
      "        - Be concise: 1 short paragraph, maximum 2-3 sentences.\n",
      "        - Do not invent facts, do not speculate, and do not use external knowledge.\n",
      "        - If multiple relevant points exist in Context, synthesize them clearly.\n",
      "        - Preserve any important terminology from the Context.\n",
      "\n",
      "        Context:\n",
      "        Virat Kohli is a Cricketer.\n",
      "\n",
      "        Query:\n",
      "        What sport does Virat play?\n",
      "\n",
      "        Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Store\n",
    "from langchain_core.prompts import PromptTemplate, load_prompt\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        You are a helpful assistant that answers strictly based on the provided context.\n",
    "\n",
    "        Guidelines:\n",
    "        - Use only the information in the Context to answer the Query.\n",
    "        - If the answer is not present in the Context, say \"I don't have enough information in the provided context to answer that.\"\n",
    "        - Be concise: 1 short paragraph, maximum 2-3 sentences.\n",
    "        - Do not invent facts, do not speculate, and do not use external knowledge.\n",
    "        - If multiple relevant points exist in Context, synthesize them clearly.\n",
    "        - Preserve any important terminology from the Context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Query:\n",
    "        {query}\n",
    "\n",
    "        Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "prompt_template.save('test.json')\n",
    "\n",
    "# Load\n",
    "from langchain_core.prompts import load_prompt\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "\n",
    "\n",
    "prompt_template = load_prompt('test.json')\n",
    "\n",
    "# prompt: PromptValue\n",
    "prompt: PromptValue = prompt_template.invoke({\n",
    "    'context': 'Virat Kohli is a Cricketer.',\n",
    "    'query': 'What sport does Virat play?'\n",
    "})\n",
    "\n",
    "print(prompt.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d3519-192a-4c64-943d-13d80317fcf2",
   "metadata": {},
   "source": [
    "**Prompt Template is tightly coupled in Langchain ecosystem. It is a runnable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99e944",
   "metadata": {},
   "source": [
    "## LangChain Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8b41f-1733-4aa7-812e-759af9d61108",
   "metadata": {},
   "source": [
    "### Types of LangChain Messages\n",
    "\n",
    "1. System Message\n",
    "2. Human Message\n",
    "3. AI Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a67bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  test 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob: 1 tset\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  test 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob: 2 tset\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant! Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='test 1', additional_kwargs={}, response_metadata={}), '1 tset', HumanMessage(content='test 2', additional_kwargs={}, response_metadata={}), '2 tset']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import load_prompt\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "chat_history = [\n",
    "    SystemMessage(content=\"You are a helpful assistant! Your name is Bob.\"),\n",
    "]\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id='',\n",
    "#     task='text-generation'\n",
    "# )\n",
    "\n",
    "# model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "class FakeChatModel:\n",
    "    def invoke(self, chat_history: List[HumanMessage]):\n",
    "        return AIMessage(content=chat_history[-1].content[::-1])\n",
    "\n",
    "\n",
    "model = FakeChatModel()\n",
    "\n",
    "while True:\n",
    "    query = input('Enter Your Query: ')\n",
    "    if query=='exit':\n",
    "        print(chat_history)\n",
    "        break\n",
    "    else:\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        response = model.invoke(chat_history)\n",
    "        print(f'Bob: {response.content}')\n",
    "        chat_history.append(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f62d2",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64e3be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful Cricket expert\n",
      "Human: Explain me this, in Batting, topic Strike Rate\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful {expert} expert'),\n",
    "    ('human', 'Explain me this, in {context}, topic {topic}')\n",
    "])\n",
    "\n",
    "prompt = chat_prompt_template.invoke({\n",
    "    \"expert\": 'Cricket',\n",
    "    \"context\": 'Batting',\n",
    "    \"topic\": \"Strike Rate\"\n",
    "})\n",
    "\n",
    "print(prompt.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1bbe4",
   "metadata": {},
   "source": [
    "## MessagePlaceholder\n",
    "\n",
    "If the user comes to a particular chat which he had in the past and asks about something, then we need to retrieve what conversation user has had in the part in that chat history and feed that to llm before answering user's query. That's where MessagePlaceholder comes into the play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22f81632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant\n",
      "Human: What is the status of my appointment\n",
      "AI: It is scheduled on 24th September 2025\n",
      "Human: Ok\n",
      "Human: When have you schedules my appointment with the doc?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human', '{query}')\n",
    "])\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content='What is the status of my appointment'),\n",
    "    AIMessage(content='It is scheduled on 24th September 2025'),\n",
    "    HumanMessage(content='Ok')\n",
    "]\n",
    "\n",
    "prompt = chat_prompt_template.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    'query': 'When have you schedules my appointment with the doc?'\n",
    "})\n",
    "\n",
    "print(prompt.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

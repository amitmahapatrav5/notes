{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d320dca2",
   "metadata": {},
   "source": [
    "# LangChain Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2e267-ce99-4dcd-9ade-8fe9e727a6bf",
   "metadata": {},
   "source": [
    "## Need of LangChain Model Component\n",
    "\n",
    "- We have various LLM Providers(OpenAI, Google, Anthropic etc), and interaction with each provider's model via api is different.\n",
    "- Langchain's Model component provides a common interface using which we can connect with any of these LLM Providers.\n",
    "\n",
    "## Types of LangChain Model\n",
    "\n",
    "1. Language Model (Text -> Text)\n",
    "   - LLM\n",
    "   - Chat Model\n",
    "2. Embedding Model (Text -> Vector)\n",
    "\n",
    "## Language Model (LLM vs Chat Model)\n",
    "\n",
    "### LLM\n",
    "\n",
    "- LLM Models are general Purpose models. You can use them for _Text Generation_, _Summarization_, _Code Generation_ etc\n",
    "- These Models take raw text as input and raw text as output\n",
    "- Legacy Models. Not used anymore.\n",
    "- **How these are trained:** General text Corpora(books and wikipedia data etc)\n",
    "- **Memory & Context:** No built-in Memory Support\n",
    "- **Role Awareness:** No understanding of roles.\n",
    "- **Example Models:** GPT-3, Llama-2.7B, Mistral-7B etc\n",
    "\n",
    "### Chat Models\n",
    "\n",
    "- Specialized for Conversational Task\n",
    "- Takes a sequence of Messages as input and Chat Messages(this is not plain text) as output.\n",
    "- Newer Model\n",
    "- **How these are trained:** After Base Models(LLMs) are prepared, they are fine-tuned on _Chat Dataset_(like dialogues, conversations etc)\n",
    "- **Memory & Context:** Supports Structured Conversation History\n",
    "- **Role Awareness:** Understands 'system', 'user' and 'assistant' roles.\n",
    "- **Example Models:** GPT-3.5-turbo, GPT-4, Llama-2-Chat, Mistral-Instruct etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5238ae-4dfa-44bd-b959-4561c46598d8",
   "metadata": {},
   "source": [
    "## Implementation - OpenAI - LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e4094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAI\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# # NOTE: We are not passing API key explicitly.\n",
    "# # Because the constructor will automatically fetch the API key from environment.\n",
    "# # Provided the key is stored against a specific key name\n",
    "# # In this case that is OPENAI_API_KEY\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "# llm = OpenAI(model='gpt-3.5-turbo-instruct')\n",
    "\n",
    "\n",
    "# result = llm.invoke('Gimme just the name of top 3 alternatives of langchain.') # This is a raw text\n",
    "\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b2aff-7d56-4c63-86f6-d2dd4e08e767",
   "metadata": {},
   "source": [
    "**temperature param: When temperature=0, then for the given input, LLM is going to generate the same output all the time**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811acc29",
   "metadata": {},
   "source": [
    "## Implementation - OpenAI - Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf138ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- LlamaIndex  \\n- LangFlow  \\n- Chroma  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OPENAI_API_KEY\n",
    "model = ChatOpenAI(\n",
    "    base_url='https://openrouter.ai/api/v1',\n",
    "    model='openai/gpt-oss-20b:free'\n",
    ")\n",
    "\n",
    "response = model.invoke('Gimme just the name of top 3 alternatives of langchain.')\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875709c",
   "metadata": {},
   "source": [
    "## Implementation - HuggingFace - API - Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856d9d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LlamaIndex  \\nHaystack  \\nLangGraph'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# HUGGINGFACEHUB_API_TOKEN\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='openai/gpt-oss-20b',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "result = model.invoke('Gimme just the name of top 3 alternatives of langchain.')\n",
    "\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c18855",
   "metadata": {},
   "source": [
    "## Implementation - HuggingFace - Local - Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef48273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     repo_id='openai/gpt-oss-20b',\n",
    "#     task='text-generation'\n",
    "# )\n",
    "\n",
    "# model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# result = model.invoke('Gimme just the name of top 3 alternatives of langchain.')\n",
    "\n",
    "# result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b63cd",
   "metadata": {},
   "source": [
    "## Implementation - OpenAI - Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d07e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078c8883-69a6-4e9d-9b47-47e0dedeaa77",
   "metadata": {},
   "source": [
    "## Implementation - HuggingFace - API - Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccfee02-c3c2-4183-92cc-d9883ff6cc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "\n",
    "# HUGGINGFACEHUB_API_TOKEN\n",
    "load_dotenv()\n",
    "\n",
    "embedding_model = HuggingFaceEndpointEmbeddings(\n",
    "    repo_id='google/embeddinggemma-300m'\n",
    ")\n",
    "embedding = embedding_model.embed_query(\n",
    "    text='Gimme just the name of top 3 alternatives of langchain.'\n",
    ")\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b9625",
   "metadata": {},
   "source": [
    "## Implementation - HuggingFace - Local - Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2403-80af-40b3-9f73-4659378e7021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

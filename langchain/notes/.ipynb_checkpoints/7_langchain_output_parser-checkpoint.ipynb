{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82ebe526",
   "metadata": {},
   "source": [
    "# LangChain Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241ad7b-2dad-475a-ba79-201a97c5e604",
   "metadata": {},
   "source": [
    "- The models which by default cannot provide structured output, we need Output Parsers for them.\n",
    "- Output Parsers help convert raw LLM response into structured formats like JSON, CSV, Pydantic models and more.\n",
    "- They ensure consistency and validation.\n",
    "- You can use output parsers with both \"can\" and \"cannot\" models\n",
    "- Different Types of Output Parsers are there. Here are the most common 4\n",
    "  - `StrOutputParser`\n",
    "  - `JSONOutputParser`\n",
    "  - `StructuredOutputParser`\n",
    "  - `PydanticOutputParser`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192aeca6-8841-47a6-987e-b1d613c35ce8",
   "metadata": {},
   "source": [
    "## StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9624d-0596-46da-a8db-873592fc3f63",
   "metadata": {},
   "source": [
    "- This is the most simple output parser.\n",
    "- Used to parse the output of LLM and return in plain string\n",
    "- Mainly, when we use an LLM, it returns the output in a key called `content`. That can be fetched directly using output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39bdb5-4d02-490c-8862-5849ce38a977",
   "metadata": {},
   "source": [
    "### Why not just use result.content instead of StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68e13f-d4ec-4313-bfa6-09b77c0a69d7",
   "metadata": {},
   "source": [
    "**Without using parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbdeeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "prompt_template_1 = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Generate a 2 liner joke on {topic}\n",
    "    \"\"\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt_template_2 = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Generate the explanation for the below joke which was created on the topic {topic}\n",
    "    Joke:\n",
    "    {joke}\n",
    "    \"\"\",\n",
    "    input_variables=['joke', 'topic']\n",
    ")\n",
    "\n",
    "joke_chain = prompt_template_1 | model\n",
    "\n",
    "joke_response = joke_chain.invoke({\n",
    "    'topic': 'AI'\n",
    "})\n",
    "\n",
    "joke = joke_response.content\n",
    "\n",
    "explanation_chain = prompt_template_2 | model\n",
    "\n",
    "explanation = explanation_chain.invoke({\n",
    "    'topic': 'AI',\n",
    "    'joke': joke\n",
    "})\n",
    "\n",
    "print(explanation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f7ac4",
   "metadata": {},
   "source": [
    "**Using parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "prompt_template_1 = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Generate a 2 liner joke on {topic}\n",
    "    \"\"\",\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# The reason of variable name is {text} because StrOutputParser object hold the LLM output value in the property called text.\n",
    "# If we give any other value the pipeline will fail\n",
    "prompt_template_2 = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Generate the explanation for the below joke {text}\n",
    "    \"\"\",\n",
    "    input_variables=['text']\n",
    ")\n",
    "# Also in the prompt_template_2, if there is any other input is required, then also we have to break the chain and build it separately.\n",
    "# Something like below\n",
    "# prompt_template_2 = PromptTemplate(\n",
    "#     template=\"\"\"\n",
    "#     Generate the explanation for the below joke which is on the topic {topic}\n",
    "#     {text}\n",
    "#     \"\"\",\n",
    "#     input_variables=['topic','text']\n",
    "# )\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template_1 | model | output_parser | prompt_template_2 | model | output_parser\n",
    "\n",
    "explanation = chain.invoke({\n",
    "    'topic': 'AI'\n",
    "})\n",
    "\n",
    "print(explanation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbbb794",
   "metadata": {},
   "source": [
    "## JSONOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Generate a 2 liner joke\n",
    "    {instruction_about_format}\n",
    "    \"\"\",\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'instruction_about_format': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# response = prompt_template.invoke({}) # you have to pass an empty dict, even if there is no put required\n",
    "\n",
    "# print(response.text)\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "joke = chain.invoke({})\n",
    "\n",
    "print(joke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d182c",
   "metadata": {},
   "source": [
    "## StructuredOutputParser\n",
    "\n",
    "- JSONOutputParser does not enforce any schema.\n",
    "- StructuredOutputParser helps extract **structured JSON** from response based on predefined field schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser # WHY? because it is not very much reusable\n",
    "from langchain.output_parsers import ResponseSchema # This help in creating JSON Schema\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Need to understand a little about JSON Schema\n",
    "schema = [\n",
    "    ResponseSchema(name='fact_1', description='Fact 1 topic description'),\n",
    "    ResponseSchema(name='fact_2', description='Fact 2 topic description'),\n",
    "    ResponseSchema(name='fact_3', description='Fact 3 topic description')\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Generate 3 facts about the topic {topic}\n",
    "    {instruction_about_format}\n",
    "    \"\"\",\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'instruction_about_format': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# response = prompt_template.invoke({'topic': 'AI'}) # you have to pass an empty dict, even if there is no put required\n",
    "\n",
    "# print(response.text)\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "facts = chain.invoke({})\n",
    "\n",
    "print(facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff20359",
   "metadata": {},
   "source": [
    "## PydanticOutputParser\n",
    "\n",
    "- Issue with StructuredOutputParser is, you cannot enforce data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e821ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser # WHY? because it is frequently used\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description='Name of the person')\n",
    "    age: int = Field(gt=18, description='Age of the person')\n",
    "    city: str = Field(description='City in which the person is living')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Generate the details about a fictional person of {country}\n",
    "    {instruction_about_format}\n",
    "    \"\"\",\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'instruction_about_format': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# response = prompt_template.invoke({'country': 'India'})\n",
    "\n",
    "# print(response.text)\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "facts = chain.invoke({})\n",
    "\n",
    "print(facts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

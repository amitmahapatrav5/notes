{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c90db2",
   "metadata": {},
   "source": [
    "# LangChain Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3e40d-9cff-432e-8624-c2e00bb33003",
   "metadata": {},
   "source": [
    "## PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46631614-c4d3-41e6-a949-b95dfd0ef4fc",
   "metadata": {},
   "source": [
    "**Types of Prompt**\n",
    "\n",
    "1. Static Prompt (Raw text sent by the used. example: \"What is the capital of India\")\n",
    "2. Dynamic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00068d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are a helpful assistant that answers strictly based on the provided context.\n",
      "\n",
      "        Guidelines:\n",
      "        - Use only the information in the Context to answer the Query.\n",
      "        - If the answer is not present in the Context, say \"I don't have enough information in the provided context to answer that.\"\n",
      "        - Be concise: 1 short paragraph, maximum 2-3 sentences.\n",
      "        - Do not invent facts, do not speculate, and do not use external knowledge.\n",
      "        - If multiple relevant points exist in Context, synthesize them clearly.\n",
      "        - Preserve any important terminology from the Context.\n",
      "\n",
      "        Context:\n",
      "        Virat Kohli is a Cricketer.\n",
      "\n",
      "        Query:\n",
      "        What sport does Virat play?\n",
      "\n",
      "        Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        You are a helpful assistant that answers strictly based on the provided context.\n",
    "\n",
    "        Guidelines:\n",
    "        - Use only the information in the Context to answer the Query.\n",
    "        - If the answer is not present in the Context, say \"I don't have enough information in the provided context to answer that.\"\n",
    "        - Be concise: 1 short paragraph, maximum 2-3 sentences.\n",
    "        - Do not invent facts, do not speculate, and do not use external knowledge.\n",
    "        - If multiple relevant points exist in Context, synthesize them clearly.\n",
    "        - Preserve any important terminology from the Context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Query:\n",
    "        {query}\n",
    "\n",
    "        Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"query\"],\n",
    ")\n",
    "\n",
    "# prompt: PromptValue\n",
    "prompt: PromptValue = prompt_template.invoke({\n",
    "    'context': 'Virat Kohli is a Cricketer.',\n",
    "    'query': 'What sport does Virat play?'\n",
    "})\n",
    "\n",
    "print(prompt.to_string())\n",
    "# print(prompt.to_messages()) # HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d3e90",
   "metadata": {},
   "source": [
    "### Why not a Python F-String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359a555-2e4d-4ed3-8f35-0508af3f3895",
   "metadata": {},
   "source": [
    "**We get a built-in validation, f-string does not provide that**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f91447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Name:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello \n"
     ]
    }
   ],
   "source": [
    "name = input('Enter Your Name: ')\n",
    "print(f'Hello {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9730d-a5f0-47bc-a11c-6e3143c2b134",
   "metadata": {},
   "source": [
    "**Prompt Templates can be stored outside the codebase in a separate file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55223a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are a helpful assistant that answers strictly based on the provided context.\n",
      "\n",
      "        Guidelines:\n",
      "        - Use only the information in the Context to answer the Query.\n",
      "        - If the answer is not present in the Context, say \"I don't have enough information in the provided context to answer that.\"\n",
      "        - Be concise: 1 short paragraph, maximum 2-3 sentences.\n",
      "        - Do not invent facts, do not speculate, and do not use external knowledge.\n",
      "        - If multiple relevant points exist in Context, synthesize them clearly.\n",
      "        - Preserve any important terminology from the Context.\n",
      "\n",
      "        Context:\n",
      "        Virat Kohli is a Cricketer.\n",
      "\n",
      "        Query:\n",
      "        What sport does Virat play?\n",
      "\n",
      "        Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Store\n",
    "from langchain_core.prompts import PromptTemplate, load_prompt\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        You are a helpful assistant that answers strictly based on the provided context.\n",
    "\n",
    "        Guidelines:\n",
    "        - Use only the information in the Context to answer the Query.\n",
    "        - If the answer is not present in the Context, say \"I don't have enough information in the provided context to answer that.\"\n",
    "        - Be concise: 1 short paragraph, maximum 2-3 sentences.\n",
    "        - Do not invent facts, do not speculate, and do not use external knowledge.\n",
    "        - If multiple relevant points exist in Context, synthesize them clearly.\n",
    "        - Preserve any important terminology from the Context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Query:\n",
    "        {query}\n",
    "\n",
    "        Answer:\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "prompt_template.save('test.json')\n",
    "\n",
    "# Load\n",
    "from langchain_core.prompts import load_prompt\n",
    "from langchain_core.prompt_values import PromptValue\n",
    "\n",
    "\n",
    "prompt_template = load_prompt('test.json')\n",
    "\n",
    "# prompt: PromptValue\n",
    "prompt: PromptValue = prompt_template.invoke({\n",
    "    'context': 'Virat Kohli is a Cricketer.',\n",
    "    'query': 'What sport does Virat play?'\n",
    "})\n",
    "\n",
    "print(prompt.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d3519-192a-4c64-943d-13d80317fcf2",
   "metadata": {},
   "source": [
    "**Prompt Template is tightly coupled in Langchain ecosystem. It is a runnable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99e944",
   "metadata": {},
   "source": [
    "## LangChain Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8b41f-1733-4aa7-812e-759af9d61108",
   "metadata": {},
   "source": [
    "### Types of LangChain Messages\n",
    "\n",
    "1. System Message\n",
    "2. Human Message\n",
    "3. AI Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8a67bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  Here are 2 numbers for you 2 and 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob: Hi! I see you’ve given me the numbers 2 and 3. How can I help you with them? For example, I could add them together, multiply them, find the difference, or anything else you have in mind. Let me know what you’d like!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  add them/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob: Sure thing!  \n",
      "2 + 3 = 5.  \n",
      "\n",
      "Let me know if you’d like to do anything else with those numbers!  \n",
      "— Bob\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  multiply them\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob: 2 × 3 = **6**.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  now subtract the bigger one from smaller one and see if you get negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob: Sure! Subtracting the larger number (3) from the smaller one (2) gives:\n",
      "\n",
      "2 − 3 = −1\n",
      "\n",
      "And yes, the result is negative.  \n",
      "Let me know if you’d like to do anything else!  \n",
      "— Bob\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  ok, thanks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob: You’re welcome! If you have any more questions or need help with anything else, just let me know. Have a great day!  \n",
      "— Bob\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Your Query:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are a helpful assistant! Your name is Bob.' additional_kwargs={} response_metadata={}\n",
      "content='Here are 2 numbers for you 2 and 3' additional_kwargs={} response_metadata={}\n",
      "content='Hi! I see you’ve given me the numbers 2 and 3. How can I help you with them? For example, I could add them together, multiply them, find the difference, or anything else you have in mind. Let me know what you’d like!' additional_kwargs={} response_metadata={}\n",
      "content='add them/' additional_kwargs={} response_metadata={}\n",
      "content='Sure thing!  \\n2\\u202f+\\u202f3\\u202f=\\u202f5.  \\n\\nLet me know if you’d like to do anything else with those numbers!  \\n— Bob' additional_kwargs={} response_metadata={}\n",
      "content='multiply them' additional_kwargs={} response_metadata={}\n",
      "content='2\\u202f×\\u202f3\\u202f=\\u202f**6**.' additional_kwargs={} response_metadata={}\n",
      "content='now subtract the bigger one from smaller one and see if you get negative' additional_kwargs={} response_metadata={}\n",
      "content='Sure! Subtracting the larger number (3) from the smaller one (2) gives:\\n\\n2\\u202f−\\u202f3\\u202f=\\u202f−1\\n\\nAnd yes, the result is negative.  \\nLet me know if you’d like to do anything else!  \\n— Bob' additional_kwargs={} response_metadata={}\n",
      "content='ok, thanks' additional_kwargs={} response_metadata={}\n",
      "content='You’re welcome! If you have any more questions or need help with anything else, just let me know. Have a great day!  \\n— Bob' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import load_prompt\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "chat_history = [\n",
    "    SystemMessage(content=\"You are a helpful assistant! Your name is Bob.\"),\n",
    "]\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='openai/gpt-oss-20b',\n",
    "    task='text-generation'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "while True:\n",
    "    query = input('Enter Your Query: ')\n",
    "    if query=='exit':\n",
    "        for message in chat_history:\n",
    "            print(message)\n",
    "        break\n",
    "    else:\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        response = model.invoke(chat_history)\n",
    "        chat_history.append(AIMessage(content=response.content))\n",
    "        print(f'Bob: {response.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f62d2",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e3be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful Cricket expert\n",
      "Human: Explain me this, in Batting, topic Strike Rate\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful {expert} expert'),\n",
    "    ('human', 'Explain me this, in {context}, topic {topic}')\n",
    "])\n",
    "\n",
    "prompt = chat_prompt_template.invoke({\n",
    "    \"expert\": 'Cricket',\n",
    "    \"context\": 'Batting',\n",
    "    \"topic\": \"Strike Rate\"\n",
    "})\n",
    "\n",
    "print(prompt.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1bbe4",
   "metadata": {},
   "source": [
    "## MessagePlaceholder\n",
    "\n",
    "If the user comes to a particular chat which he had in the past and asks about something, then we need to retrieve what conversation user has had in the part in that chat history and feed that to llm before answering user's query. That's where MessagePlaceholder comes into the play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f81632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant\n",
      "Human: What is the status of my appointment\n",
      "AI: It is scheduled on 24th September 2025\n",
      "Human: Ok\n",
      "Human: When have you schedules my appointment with the doc?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human', '{query}')\n",
    "])\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content='What is the status of my appointment'),\n",
    "    AIMessage(content='It is scheduled on 24th September 2025'),\n",
    "    HumanMessage(content='Ok')\n",
    "]\n",
    "\n",
    "prompt = chat_prompt_template.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    'query': 'When have you schedules my appointment with the doc?'\n",
    "})\n",
    "\n",
    "print(prompt.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

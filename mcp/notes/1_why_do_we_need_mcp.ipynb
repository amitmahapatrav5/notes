{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1f83f2-bd54-4f81-aa8f-265ef1170711",
   "metadata": {},
   "source": [
    "# Why do we need MCP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603432a-00f1-4164-90b9-196e9c04f954",
   "metadata": {},
   "source": [
    "- 30 November 2022 - ChatGPT Released (Crossed 1 million users in 5 days and 100 million in 2 months)\n",
    "- 1st March 2023 - OpenAI release an API around the model(gpt-3.5-turbo) to public.\n",
    "- Then, Microsoft integrated copilot with its tools like Word, Excel and Powerpoint. Google integrated gemini to G-Suite.\n",
    "- Then we have First AI tools like Cursor and Perplexity.\n",
    "- So basically the tools that we were using got AI enabled.\n",
    "- The only problem was, tool1’s ai has no idea about tool2’s ai. For example, Notion AI has no idea what’s happening in Microsoft Word’s AI.\n",
    "- Basically if I wanted a task done, I have to juggle between multiple AI’s which are integrated with their corresponding products.\n",
    "- Basically we don’t have a **Unified AI** to get our work done.\n",
    "- This created a problem. Here is an example.\n",
    "    - Say I get a task to add Single Sign-On (SSO) functionality to an existing FastAPI application, and I want ChatGPT to generate the entire code for it. To do that, ChatGPT needs to understand the project context. But that information is scattered across multiple places.\n",
    "    - For example, the user story for this new feature is in JIRA, the existing codebase is in Bitbucket, and some relevant documents like architecture diagrams and specification docs are stored in OneDrive. The authentication details required for SSO are in Outlook emails. During development, if I get stuck, there’s also a Teams group where I’ve had discussions with the original project developers.\n",
    "    - Now, for ChatGPT to actually help, I have to manually copy the user story from JIRA, the relevant code from Bitbucket, the emails from Outlook, and the documents from OneDrive — and feed all of that into ChatGPT. Once I do that, it finally has the full context and can start working on the implementation.\n",
    "    - But if ChatGPT can’t solve something and I discuss it with the developers on Teams, I then have to copy and paste that entire conversation back into ChatGPT again. The same thing happens later during the testing phase — more copy-pasting, every time the context changes.\n",
    "    - I call this repetitive, frustrating process “Copy-Paste Hell”. And the root cause of it is simple: my project context is distributed across different applications.\n",
    "    - It can also be pointed out that we often end up spending more time gathering context for ChatGPT than actually working on the real problem. On top of that, we have to constantly manage what the AI has already “remembered.” For instance, if the codebase has around 50,000 lines of code along with numerous configuration and settings files, it’s nearly impossible to feed all of that to ChatGPT in a clear and manageable way.\n",
    "    - That’s what makes building a truly Unified AI so challenging — because in real-world scenarios, the project information is scattered across multiple tools and platforms.\n",
    "    - It would be far better if ChatGPT could automatically access all this information from different sources, without me having to copy and paste anything manually. And this problem — the challenge of fragmented project context — is exactly what Tools solves.\n",
    "- Now, **What is context?** Context is everything that an AI can “see” when it generates a response. It could be a conversation history or a document.\n",
    "- In the mid of 2023 OpenAI introduced function calling. It basically means, LLM will be given some function(so called tools) and from the give prompt, LLM can understand which function to call with the correct parameter.\n",
    "- Then there is the ear of Tools. People started building Tool(basically functions) to get data from Weather API, get Data from DB, get Code from GitHub, get information from Web etc. This was a revolutionary. Because now the LLM can actually do some task instead of only generating the text.\n",
    "- Now the problem I was having of manually copy-pasting the information from various platforms to ChatGPT, I don’t have to do that.\n",
    "- Now I have tools(basically one function for each tool), which I can bind with ChatGPT and now it can fetch all the relevant data from corresponding platform.\n",
    "    - Need US -> JIRA\n",
    "    - Need Code -> GitHub\n",
    "    - Need Spec -> OneDrive\n",
    "    - Need SSO Info -> Outlook\n",
    "    - Need Conversation History -> Teams\n",
    "- BUT still there is a problem: The problem of **STANDARDIZATION**\n",
    "- **Say you have N LLMs and M SaaS apple then you have to write N*M functions.**\n",
    "- The number of functions will grow drastically with a new LLM use or a new SaaS. And there are several problems\n",
    "    - For each tool there is separate auth and other app specific work.\n",
    "    - We have to also maintain these functions as well. If an API change something we have to update that for all the functions.\n",
    "    - Then there is time problem. Even though we have a new llm, still we need time to implement the functions for its integration which will take time.\n",
    "    - This is not scalable.\n",
    "- Now this standardization problem is also now solved. And the solution is **MCP** - Model Context Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c4910-6762-4c42-87fd-1d3ea1d1e9a6",
   "metadata": {},
   "source": [
    "**Question:** Using LangChain, I can write code which is vendor agnostic. So I will just have tools which I can write and then when I need to change model, I can just change the code which I are responsible to connect with the LLM.\n",
    "\n",
    "**Answer:** I think I will get the answer once I understand how MCP works at a low leverl."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

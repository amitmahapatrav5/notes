{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa94770",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Section: 7 \\\n",
    "Lecture: 48 \\\n",
    "Title: Learning rates comparison \\\n",
    "TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842132 \\\n",
    "Udemy Reference Link: \\\n",
    "Pre-Requisite:\n",
    "\n",
    "# Learning Rate Comparisons\n",
    "\n",
    "## Why Setting the correct learning rate is important?\n",
    "\n",
    "If we select a very large learning rate, then we may fall in the issue of divergence or just keep bouncing between few points and never reach the minima.\n",
    "\n",
    "However if we select a very small learning rate, the model might take very long time to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ad266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 20)\n",
    "y = x**2\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af26e1",
   "metadata": {},
   "source": [
    "![png](7_ann_48_learning_rate_comparisons_files/7_ann_48_learning_rate_comparisons_4_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = lambda x : x**2\n",
    "grad = lambda x: 2*x\n",
    "epochs = 50\n",
    "\n",
    "def learn(lr):\n",
    "    localmins = torch.zeros(epochs)\n",
    "\n",
    "    localmin = torch.tensor(0.75) # initial min\n",
    "    for epoch in range(epochs):\n",
    "        localmins[epoch] = localmin\n",
    "        localmin = localmin - lr*grad(localmin)\n",
    "    return localmins\n",
    "\n",
    "diverge_localmins = learn(1)\n",
    "converge_localmins = learn(0.01)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "axes[0].set_title('High Learning Rate - Keep bouncing between 2 points')\n",
    "axes[0].plot(x, y)\n",
    "axes[0].scatter(diverge_localmins, fx(diverge_localmins), marker='x', color='r')\n",
    "\n",
    "axes[1].set_title('Lower Learning Rate - Slow Learning')\n",
    "axes[1].plot(x, y)\n",
    "axes[1].scatter(converge_localmins, fx(converge_localmins), marker='x', color='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de116048",
   "metadata": {},
   "source": [
    "![png](7_ann_48_learning_rate_comparisons_files/7_ann_48_learning_rate_comparisons_5_0.png)\n",
    "\n",
    "## Parametric Experiment - Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [ 1, 1 ]\n",
    "B = [ 5, 1 ]\n",
    "N = 100\n",
    "\n",
    "a = torch.stack( (A[0] + torch.randn(N), A[1] + torch.randn(N)), dim=1 )\n",
    "b = torch.stack( (B[0] + torch.randn(N), B[1] + torch.randn(N)), dim=1 )\n",
    "\n",
    "data = torch.vstack((a, b))\n",
    "labels = torch.vstack( (torch.zeros(N, 1), torch.ones(N, 1)) )\n",
    "\n",
    "data.shape, labels.shape\n",
    "\n",
    "plt.scatter(data[ torch.where(labels == 0)[0], 0], data[ torch.where(labels == 0)[0], 1], marker='s', color='b', facecolor='w')\n",
    "plt.scatter(data[ torch.where(labels == 1)[0], 0], data[ torch.where(labels == 1)[0], 1], marker='s', color='g', facecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e10cb",
   "metadata": {},
   "source": [
    "![png](7_ann_48_learning_rate_comparisons_files/7_ann_48_learning_rate_comparisons_8_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7894d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNClassify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANNClassify, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "\n",
    "model = ANNClassify()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e06368",
   "metadata": {},
   "source": [
    "    ANNClassify(\n",
    "      (stack): Sequential(\n",
    "        (0): Linear(in_features=2, out_features=1, bias=True)\n",
    "        (1): ReLU()\n",
    "        (2): Linear(in_features=1, out_features=1, bias=True)\n",
    "        (3): Sigmoid()\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11558567",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "def train(lr):\n",
    "    model = ANNClassify()\n",
    "\n",
    "    loss_func = nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = torch.zeros(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        yHat = model(data)\n",
    "\n",
    "        loss = loss_func(yHat, labels)\n",
    "        losses[epoch] = loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    prediction = model(data)\n",
    "    accuracy = torch.mean(((prediction>0.5) == labels.bool()).float())*100\n",
    "\n",
    "    return losses, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d3067",
   "metadata": {},
   "source": [
    "## Experiment Learning Rate vs Accuracy and Epochs vs Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = torch.linspace(0.001, 0.1, 40)\n",
    "\n",
    "allLosses = torch.zeros((len(lrs), epochs))\n",
    "accuracies = torch.zeros(len(lrs))\n",
    "\n",
    "for i in range(len(lrs)):\n",
    "    losses, accuracy = train(lrs[i])\n",
    "\n",
    "    allLosses[i, :] = losses\n",
    "    accuracies[i] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Experiment Findings\n",
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Learning Rate vs Accuracy\n",
    "axes[0].plot(lrs.detach(), accuracies.detach(), marker='s', linestyle='-', markerfacecolor='w')\n",
    "axes[0].set_xlabel('Learning Rate')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title(f'Accuracy(MAX={accuracies.max()}%, MIN={accuracies.min()}%) by Learning Rate')\n",
    "\n",
    "# Epochs vs Losses\n",
    "for i, lr in enumerate(lrs):\n",
    "    axes[1].plot(range(len(allLosses[i])), allLosses[i].detach(), linestyle='-')\n",
    "axes[1].set_xlabel('Loss')\n",
    "axes[1].set_ylabel('Epochs')\n",
    "axes[1].set_title('Losses by Learning Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1811b",
   "metadata": {},
   "source": [
    "![png](7_ann_48_learning_rate_comparisons_files/7_ann_48_learning_rate_comparisons_13_0.png)\n",
    "\n",
    "### Why model is either behaving very good or very poor?\n",
    "\n",
    "It's really very difficult to say. There could be various possibility like\n",
    "\n",
    "1. May be the model is not complex enough\n",
    "2. May be the problem statement is a linear one and we used a non-linear model etc\n",
    "\n",
    "Also it is very difficult to visualize whether the model is getting stuck in a local minima in case of poor performance. Because we are trying to minimize the loss function in 6(3 weights + 2 biases) + 1 (output feature) dimensions and it si not possible to visualize."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

7_ann_40_the_perceptron_and_ann_architecture.md

# Reference

Section: 7 \
Lecture: 40 \
Title: The perceptron and ANN architecture \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842114 \
Udemy Reference Link: \
Pre-Requisite:

# Linear and Non Linear ANN Model

**Linear models only solve linearly separable problems** \
**Model is considered Linear, if the operation it is performing is only addition or multiplication** \
**If the Model is performing anything else, then it is Non-Linear Model** \
**I think the only component that can be non Linear is activation function**

**We should not use Linear Models for Non Linear Problems** \
**Similarly, we should not use Non Linear Models for Linear Problems**

```python
import torch
from torch import nn
```

```python
# Example of Linear Model
class  LinearANN(nn.Module):
    def __init__(self):
        super(LinearANN, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(3, 1)
        )
LinearANN()
```

    LinearANN(
      (stack): Sequential(
        (0): Linear(in_features=3, out_features=1, bias=True)
      )
    )

```python
# Example of Non-Linear Model
class NonLinearANN(nn.Module):
    def __init__(self):
        super(NonLinearANN, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(3, 1),
            nn.Sigmoid() # This is the non Linear Component
        )
NonLinearANN()
```

    NonLinearANN(
      (stack): Sequential(
        (0): Linear(in_features=3, out_features=1, bias=True)
        (1): Sigmoid()
      )
    )

**Linearly Separable problem: Where the separation can be performed by a line or plane or hyperplane**

```python
# torch.randn()
```

**Majority math that happens in any ANN is $\sigma(X^{T}W)$**

**Why do we need bias?**

Equations like $aX + bY + ... + cZ = 0$ always passes through origin. \
But equations like $aX + bY + ... + cZ + bias = 0$ does not, and hence the advantage as shown in below figure. \
However, it is possible to mean center all the points so that we will always have the separating hyper-plane pass through origin. But this is not something we always go for. I am not exactly clear on the scenario of non-linear equation

```python
import matplotlib.pyplot as plt

nPerClust = 100
blur = 1

# First diagram coordinates
A1 = [3, 8]
B1 = [8, 3]

# Second diagram coordinates (existing)
A2 = [1, 1]
B2 = [6, 6]

# Generate data for the first diagram
a1 = torch.stack((A1[0] + torch.randn(nPerClust) * blur, A1[1] + torch.randn(nPerClust) * blur))
b1 = torch.stack((B1[0] + torch.randn(nPerClust) * blur, B1[1] + torch.randn(nPerClust) * blur))

# Generate data for the second diagram
a2 = torch.stack((A2[0] + torch.randn(nPerClust) * blur, A2[1] + torch.randn(nPerClust) * blur))
b2 = torch.stack((B2[0] + torch.randn(nPerClust) * blur, B2[1] + torch.randn(nPerClust) * blur))

# True labels for both diagrams
labels1 = torch.cat((torch.zeros(nPerClust, 1), torch.ones(nPerClust, 1)))
labels2 = torch.cat((torch.zeros(nPerClust, 1), torch.ones(nPerClust, 1)))

# Concatenate into matrices
data1 = torch.cat((a1, b1), dim=1).T
data2 = torch.cat((a2, b2), dim=1).T

# Create subplots
fig, axs = plt.subplots(1, 2, figsize=(10, 5))

# No Need of Bias for this
axs[0].plot(data1[labels1.squeeze() == 0, 0], data1[labels1.squeeze() == 0, 1], 'bs', label='Cluster A')
axs[0].plot(data1[labels1.squeeze() == 1, 0], data1[labels1.squeeze() == 1, 1], 'ko', label='Cluster B')
axs[0].set_title('Separating Line Passes through Origin')
axs[0].legend()

# Need Bias for this
axs[1].plot(data2[labels2.squeeze() == 0, 0], data2[labels2.squeeze() == 0, 1], 'bs', label='Cluster A')
axs[1].plot(data2[labels2.squeeze() == 1, 0], data2[labels2.squeeze() == 1, 1], 'ko', label='Cluster B')
axs[1].set_title('Separating Line cannot pass through Origin')
axs[1].legend()

# Show the plots
plt.tight_layout()
plt.show()
```

![png](7_ann_40_the_perceptron_and_ann_architecture_files/7_ann_40_the_perceptron_and_ann_architecture_10_0.png)

```python

```

---

7_ann_41_geometric_view_of_ann.md

# Reference

Section: 7 \
Lecture: 41 \
Title: A geometric view of ANNs \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842116 \
Udemy Reference Link: \
Pre-Requisite:

Feature Space is a geometric representation of the data, where each feature is an axis and each observation is a coordinate. In practice, we have very high dimension of feature space.

Model output can be of 2 types, categorical or continuous
Categorical:
Input Features: Sleep Duration, Study Duration
Output: Pass/Fail

Continuous:
Input Features: Sleep Duration, Study Duration
Output: Marks Secured

```python

```

---

7_ann_42_ann_math_part_1\*(forward_pass).md

# Reference

Section: 7 \
Lecture: 42 \
Title: ANN math part 1 (forward prop) \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842118 \
Udemy Reference Link: \
Pre-Requisite:

# Basic Mathematics that happens in a single Neuron

## Math that happens in every neuron

In every Neuron, this is what basically happens \
$Y = \sigma( bias + \sum(X^T W) )$ \
But we can write the same thing as $Y = \sigma( \sum(X^T W) )$ \
HOW ?

$Y = \sigma( bias + \sum(X^T W) )$ \
$Y = \sigma( 1*bias + \sum(X^T W) )$ \
$Y = \sigma( 1*W_0 + \sum(X^T W) )$ \
$Y = \sigma( 1*W_0 + X_1W_1 + X_2W_2 + ... + X_nW_n )$ \
$Y = \sigma( X_0*W_0 + X_1W_1 + X_2W_2 + ... + X_nW_n )$ \
$Y = \sigma( \sum(X^T W) )$

## Why do we need activation function

**This is mainly happeing because, if you observe the heatmaps carefully, \
The left figure, where we have not used the activation function, the band value is in range from -10 to 12 \
But in the right figure, where we have used the activation function, the band value is in range from 0 to 1. \
So, in a way, we are kind of transforming the coordinate.**

```python
import torch
import matplotlib.pyplot as plt

def func_linear(x):
    return 1 - 2*x[:, 0] + x[:, 1]

def func_nonlinear(x):
    return torch.sigmoid(1 - 2*x[:, 0] + x[:, 1])

start, end = -4, 4
nCount = 100

# Generate a grid of points
x1 = torch.linspace(start, end, nCount)
x2 = torch.linspace(start, end, nCount)
x1_grid, x2_grid = torch.meshgrid(x1, x2, indexing='xy')
x_grid = torch.stack([x1_grid.flatten(), x2_grid.flatten()], dim=1)

# Compute the function values
z_linear = func_linear(x_grid).reshape(nCount, nCount)
z_nonlinear = func_nonlinear(x_grid).reshape(nCount, nCount)

# Convert to numpy for plotting
x1_np = x1_grid.numpy()
x2_np = x2_grid.numpy()
z_linear_np = z_linear.numpy()
z_nonlinear_np = z_nonlinear.numpy()

# Create the heatmaps
fig, axs = plt.subplots(1, 2, figsize=(12, 4))

# Linear function heatmap
contour1 = axs[0].contourf(x1_np, x2_np, z_linear_np, levels=50, cmap='viridis')
axs[0].set_title('Linear Function Heatmap')
axs[0].set_xlabel('x1')
axs[0].set_ylabel('x2')
fig.colorbar(contour1, ax=axs[0], label='Function Value')
axs[0].axhline(0, color='black', linewidth=0.5, ls='--')
axs[0].axvline(0, color='black', linewidth=0.5, ls='--')
axs[0].grid(color='gray', linestyle='--', linewidth=0.5)

# Nonlinear function heatmap
contour2 = axs[1].contourf(x1_np, x2_np, z_nonlinear_np, levels=50, cmap='viridis')
axs[1].set_title('Nonlinear Function Heatmap (with Activation)')
axs[1].set_xlabel('x1')
axs[1].set_ylabel('x2')
fig.colorbar(contour2, ax=axs[1], label='Function Value')
axs[1].axhline(0, color='black', linewidth=0.5, ls='--')
axs[1].axvline(0, color='black', linewidth=0.5, ls='--')
axs[1].grid(color='gray', linestyle='--', linewidth=0.5)

plt.tight_layout()
plt.show()

```

![png](7_ann_42_ann_math_part_1_%28forward_pass%29_files/7_ann_42_ann_math_part_1_%28forward_pass%29_6_0.png)

Most Common Activation Functions are

1. Sigmoid (Often used in the nodes in the output layer of the model)
2. Hyperbolic Tangent (Often used in the nodes in the middle of the model)
3. ReLU (Often used in the nodes in the middle of the model)

There are lot of other activation functions as well. \
Also, many activation functions are just variants of these 3.

```python

```

---

7_ann_43_ann_math_part_2\*(error_loss_cost).md

# Reference

Section: 7 \
Lecture: 43 \
Title: ANN math part 2 (errors, loss, cost) \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842120 \
Udemy Reference Link: \
Pre-Requisite:

# Loss Functions and Cost Function

## Loss Function

Most Commonly used Loss Functions are Mean Squared Error and Cross Entropy Error.
There are lots of other errors, but majority are basically variations of these 2 types of Loss Function.

**Mean Squared Error**

- This error function is mainly used when the model predicts continuous data
- Example: House Price Prediction, Temprature Prediction etc
- $ L = \frac{1}{2} ( \hat{y} - y )^{2} $

**Cross Entropy Error**

- This error function is used when the model predicts probability
- Example: Text Sentiment Classification, Digit Recognization etc
- $ L = -( y _ log ({\hat{y}}) + (1-y) _ log(1 - \hat{y}) ) $

## Loss Function Vs Cost Function

- Cost Function is literally just the average of losses for all the training data.
- We calculate the loss for every single data point using loss function. And cost function is just the average of all those losses.
- $ C = \frac{1}{n} \sum\_{i=1}^{n} L(\hat{y}, y)$
- The entire goal of deep learning is **find the weights such that it minimizes the cost function.**

### Why train on cost but not loss

- Training the model on each sample is time consuming and may lead to overfitting.
- Also averaging over too many sample, may decrease the sensitivity.
- The best approach is to train the model in "batches" of samples.

#### Example

- Say we have 2000 Samples in training data.
- Calculating Loss for each sample and then chainging the weights will overfit the model
- Calculating Cost from Loss of each sample and the changing the weights might decrease the sensitivity of the model
- Best way is, create batches, each having say 20 samples and calculate the cost by averaging the loss of each sample in the batch and change the weights.

```python

```

```python

```

---

7*ann_44_ann_math_part_3*(backprop).md

# Reference

Section: 7 \
Lecture: 44 \
Title: ANN math part 3 (backprop) \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842124 \
Udemy Reference Link: \
Pre-Requisite:

# Back Propagation

Formula: $ W = W - \eta \* \frac{\delta{CF}}{\delta{W}} $

We are trying to minimize the cost function with respect to Weights

So in 2D, Dimension 1 is Weight and Dimension 2 is Cost Function Value and we are trying to find out, for which Weight, W, Cost Function value is minimum.

$ \frac{\delta{CF}}{\delta{W}} $

$ \frac{\delta{ \sigma { ( X^{T}W, \hat{Y} ) } } }{\delta{W}} $

```python

```

---

7_ann_45_ann_for_regression.md

# Reference

Section: 7 \
Lecture: 45 \
Title: ANN for regression \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842126 \
Udemy Reference Link: \
Pre-Requisite:

# ANN for Regression

```python
import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
```

```python
# Generate Data
size = 50

x = torch.randn(size,1)
y = x + torch.randn(size,1)/2
# torch.randn(size,1)/2 is a noise. Why?, because it is not constant like y-intercept
x.shape, y.shape
```

    (torch.Size([50, 1]), torch.Size([50, 1]))

```python
# Plot
plt.plot(x,y,'s', label='Training Data')
plt.legend()
plt.show()
```

![png](7_ann_45_ann_for_regression_files/7_ann_45_ann_for_regression_4_0.png)

```python
class ANNRegression(nn.Module):
    def __init__(self):
        super(ANNRegression, self).__init__()
        self. stack = nn.Sequential(
            nn.Linear(1, 1)
        )
    def forward(self, data):
        return self.stack(data)
model = ANNRegression()
```

```python
epochs = 100
loss_func = nn.MSELoss()
learning_rate = 0.01
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```

```python
# training
losses = torch.zeros(epochs)

for epoch in range(epochs):
    yHat = model(x)

    loss = loss_func(y, yHat)
    losses[epoch] = loss

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

```python
# Measure Model Performance
prediction = model(x)
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

axes[0].plot(x, y, 's', color='b', label='Training Data')
axes[0].plot(x, prediction.detach(), 'o', color='g', label='Model Prediction')
axes[0].set_xlabel('x')
axes[0].set_ylabel('y')
axes[0].set_title('Reality vs Prediction')
axes[0].legend()

axes[1].scatter(range(epochs), losses.detach(), color='b')
axes[1].set_xlabel('epochs')
axes[1].set_ylabel('cost')
axes[1].set_title('Loss Per Epoch')
plt.show()
```

![png](7_ann_45_ann_for_regression_files/7_ann_45_ann_for_regression_8_0.png)

### If DL is so great, why don't we all switch to DL models instead of traditional statistical models?

Traditional statistical models tend to work better on smaller datasets, are better mathematically characterize (e.g. guaranteed optimal solutions) and are more interpretable. But DL models, does not provide any guarantee the optimal solution, but proceed towards a good solution.

---

7_ann_46_code_challenge_manipulate_regression_slopes.md

# Reference

Section: 7 \
Lecture: 46 \
Title: CodeChallenge: manipulate regression slopes \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842128 \
Udemy Reference Link: \
Pre-Requisite:

# Parametric Experiment - Slope vs Loss and Slope vs Performance

```python
import torch
from torch import nn
import matplotlib.pyplot as plt
```

```python
class ANNRegression(nn.Module):
    def __init__(self):
        super(ANNRegression, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(1, 1),
                nn.ReLU(),
            nn.Linear(1, 1)
        )
    def forward(self, X):
        return self.stack(X)

ANNRegression()
```

    ANNRegression(
      (stack): Sequential(
        (0): Linear(in_features=1, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
      )
    )

```python
def build_and_train(X, y):

    # built the model
    model = ANNRegression()

    loss_func = nn.MSELoss()
    learning_rate = 0.01
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

    # train the model
    epochs = 500
    losses = torch.zeros(epochs)
    for epoch in range(epochs):
        # feed forward
        yHat = model(X)

        # compute loss
        loss = loss_func(yHat, y)
        losses[epoch] = loss

        # back propagate
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    prediction = model(X)
    return prediction, losses
```

```python
def generate_data(m):
    N = 50

    X = torch.randn(N, 1)
    y = m*X + torch.randn(N, 1)/2

    return X, y
```

```python
# Test everything once
X, y = generate_data(m=1)
prediction, losses = build_and_train(X, y)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Reality vs Model Prediction
axes[0].scatter(X, y, color='b', marker='+', label='Reality')
axes[0].scatter(X, prediction.detach(), marker='*', color='g', label='Model Prediction')
axes[0].set_xlabel('X')
axes[0].set_ylabel('Y')
axes[0].set_title('Reality vs Model Prediction')
axes[0].legend()

# epoch vs loss
axes[1].plot(range(len(losses)), losses.detach(), color='m', marker='o')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].set_title('Epoch vs Loss')

plt.show()
```

![png](7_ann_46_code_challenge_manipulate_regression_slopes_files/7_ann_46_code_challenge_manipulate_regression_slopes_6_0.png)

```python
# Parametric Experiment
slopes = torch.linspace(-2, 2, 21)
exps = 50

result = torch.zeros(len(slopes), exps, 2)

for slp in range(len(slopes)):
    for exp in range(exps):
        X, y = generate_data(slopes[slp])
        prediction, losses = build_and_train(X, y)

        # store the performance and final loss
        result[slp, exp, 0] = torch.corrcoef(torch.vstack((prediction.T, y.T)))[0, 1]
        result[slp, exp, 1] = losses[-1]
result[torch.isnan(result)]=0
```

```python
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].plot(slopes.detach(), torch.mean(result[:, :, 0], dim=1).detach(), marker='o', color='b')
axes[0].set_title('Slope Vs Performance')
axes[0].set_xlabel('Slope')
axes[0].set_ylabel('co-relation between reality and prediction')

axes[1].plot(slopes.detach(), torch.mean(result[:, :, 1], dim=1).detach(), marker='s', color='m')
axes[1].set_title('Slope Vs Loss')
axes[1].set_xlabel('Slope')
axes[1].set_ylabel('Last Training Epoch Model Loss')

plt.show()
```

![png](7_ann_46_code_challenge_manipulate_regression_slopes_files/7_ann_46_code_challenge_manipulate_regression_slopes_8_0.png)

### Why Performance is very low in case of slope=0?

When slope=0, the model does not get more relation between x and y. However, in case of other slopes, the model could learn the relation between the x and y. You can see the same from below diagram.

```python
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

X, y = generate_data(m=0)
axes[0].scatter(X, y)

X, y = generate_data(m=2)
axes[1].scatter(X, y)

plt.show()
```

![png](7_ann_46_code_challenge_manipulate_regression_slopes_files/7_ann_46_code_challenge_manipulate_regression_slopes_10_0.png)

### Why Loss is very low in case of slope=0 and high in other cases?

This is because, in case of higher slopes, the variance of y is higher, but in case where slope is 0, the variance is lower. Normalization of data can solve this issue. In the below diagram, when slope=0, the variance is from -1 to 1, but when slope=2, variance is from -4 to 1.

```python
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

X, y = generate_data(m=0)
axes[0].scatter(X, y)

X, y = generate_data(m=2)
axes[1].scatter(X, y)

plt.show()
```

![png](7_ann_46_code_challenge_manipulate_regression_slopes_files/7_ann_46_code_challenge_manipulate_regression_slopes_12_0.png)

---

7_ann_47_ann_for_classifying_qwerties.md

# Reference

Section: 7 \
Lecture: 47 \
Title: ANN for classifying qwerties \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842130 \
Udemy Reference Link: \
Pre-Requisite:

# ANN for Classification

```python
import torch
from torch import nn

from matplotlib import pyplot as plt
```

```python
# Prepare data
N = 100
A = [ 1, 1 ]
B = [ 5, 1 ]

a = torch.vstack( ( A[0] + torch.randn(N), A[1] + torch.randn(N) ) )
b = torch.vstack( ( B[0] + torch.randn(N), B[1] + torch.randn(N) ) )

data = torch.vstack( ( a.T, b.T ) )
labels = torch.vstack( ( torch.zeros(N, 1), torch.ones(N, 1) ) )

plt.scatter(data [ torch.where(labels==0)[0], 0 ], data [ torch.where(labels==0)[0], 1 ], marker='s', color='b', facecolor='w')
plt.scatter(data [ torch.where(labels==1)[0], 0 ], data [ torch.where(labels==1)[0], 1 ], marker='s', color='g', facecolor='w')
plt.show()
```

![png](7_ann_47_ann_for_classifying_qwerties_files/7_ann_47_ann_for_classifying_qwerties_3_0.png)

```python
# Building Model
class ANNClassify(nn.Module):
    def __init__(self):
        super(ANNClassify, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(2, 1),
            nn.ReLU(),
            nn.Linear(1, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.stack(x)

model = ANNClassify()
model
```

    ANNClassify(
      (stack): Sequential(
        (0): Linear(in_features=2, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): Sigmoid()
      )
    )

```python
# Metaparameter Set Up
learning_rate = 0.01

loss_func = nn.BCELoss()

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
```

```python
# Training Model
epochs = 2000
losses = torch.zeros(epochs)

for epoch in range(epochs):

    # forward pass
    yHat = model(data)

    # Calculating Loss
    loss = loss_func(yHat, labels)
    losses[epoch] = loss

    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

```python
# Evaluating Model Performance
prediction = model(data)

prediction [ torch.where(prediction > 0.5) ] = 1.
prediction [ torch.where(prediction <= 0.5) ] = 0.

performance = torch.corrcoef ( torch.vstack( ( prediction.T, labels.T ) ) )[0, 1].detach()*100

plt.scatter(data[ torch.where(labels == 1.)[0], 0 ], data[ torch.where(labels == 1.)[0], 1 ], marker='s', color='g', facecolor='w')
plt.scatter(data[ torch.where(labels == 0.)[0], 0 ], data[ torch.where(labels == 0.)[0], 1 ], marker='s', color='b', facecolor='w')
plt.scatter(data[ torch.where(prediction != labels)[0], 0 ], data[ torch.where(prediction != labels)[0], 1 ], marker='x', color='r')
plt.title(f'Performance = {torch.round(performance)} %')
plt.show()
```

![png](7_ann_47_ann_for_classifying_qwerties_files/7_ann_47_ann_for_classifying_qwerties_7_0.png)

### Why ReLU is used in the internal layers but sigmoid in the output layer?

ReLU is commonly used in the internal layers of a neural network because it helps to mitigate the vanishing gradient(**Need to knaow what is this**) problem.
Sigmoid function is used in the output layer because it outputs values between 0 and 1, which is suitable for binary classification tasks.

### Why BCELoss is used instead of MSELoss?

This problem is a BCELoss problem. What does that mean?

### How to interpret the loss function?

If you can see here, with 1500 epochs, the curve has still not yet asymptoted. So the model can still learn. May be here we can increase the number of epochs or vary some other metaparameters to get to that phase.

```python
plt.plot( range(1, epochs+1), losses.detach(), marker='o', markerfacecolor='w' )
plt.show()
```

![png](7_ann_47_ann_for_classifying_qwerties_files/7_ann_47_ann_for_classifying_qwerties_11_0.png)

---

7_ann_48_learning_rate_comparisons.md

# Reference

Section: 7 \
Lecture: 48 \
Title: Learning rates comparison \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842132 \
Udemy Reference Link: \
Pre-Requisite:

# Learning Rate Comparisons

## Why Setting the correct learning rate is important?

If we select a very large learning rate, then we may fall in the issue of divergence or just keep bouncing between few points and never reach the minima.

However if we select a very small learning rate, the model might take very long time to learn.

```python
import torch
from matplotlib import pyplot as plt
```

```python
x = torch.linspace(-1, 1, 20)
y = x**2

plt.plot(x, y)
plt.show()
```

![png](7_ann_48_learning_rate_comparisons_files/7_ann_48_learning_rate_comparisons_4_0.png)

```python
fx = lambda x : x**2
grad = lambda x: 2*x
epochs = 50

def learn(lr):
    localmins = torch.zeros(epochs)

    localmin = torch.tensor(0.75) # initial min
    for epoch in range(epochs):
        localmins[epoch] = localmin
        localmin = localmin - lr*grad(localmin)
    return localmins

diverge_localmins = learn(1)
converge_localmins = learn(0.01)

fig, axes = plt.subplots(1, 2, figsize=(12,5))
axes[0].set_title('High Learning Rate - Keep bouncing between 2 points')
axes[0].plot(x, y)
axes[0].scatter(diverge_localmins, fx(diverge_localmins), marker='x', color='r')

axes[1].set_title('Lower Learning Rate - Slow Learning')
axes[1].plot(x, y)
axes[1].scatter(converge_localmins, fx(converge_localmins), marker='x', color='g')

plt.show()
```

![png](7_ann_48_learning_rate_comparisons_files/7_ann_48_learning_rate_comparisons_5_0.png)

## Parametric Experiment - Learning Rate

```python
import torch
from torch import nn

from matplotlib import pyplot as plt
```

```python
A = [ 1, 1 ]
B = [ 5, 1 ]
N = 100

a = torch.stack( (A[0] + torch.randn(N), A[1] + torch.randn(N)), dim=1 )
b = torch.stack( (B[0] + torch.randn(N), B[1] + torch.randn(N)), dim=1 )

data = torch.vstack((a, b))
labels = torch.vstack( (torch.zeros(N, 1), torch.ones(N, 1)) )

data.shape, labels.shape

plt.scatter(data[ torch.where(labels == 0)[0], 0], data[ torch.where(labels == 0)[0], 1], marker='s', color='b', facecolor='w')
plt.scatter(data[ torch.where(labels == 1)[0], 0], data[ torch.where(labels == 1)[0], 1], marker='s', color='g', facecolor='w')
plt.show()
```

![png](7_ann_48_learning_rate_comparisons_files/7_ann_48_learning_rate_comparisons_8_0.png)

```python
class ANNClassify(nn.Module):
    def __init__(self):
        super(ANNClassify, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(2, 1),
            nn.ReLU(),
            nn.Linear(1, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.stack(x)

model = ANNClassify()
model
```

    ANNClassify(
      (stack): Sequential(
        (0): Linear(in_features=2, out_features=1, bias=True)
        (1): ReLU()
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): Sigmoid()
      )
    )

```python
epochs = 1000

def train(lr):
    model = ANNClassify()

    loss_func = nn.BCELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)

    losses = torch.zeros(epochs)

    for epoch in range(epochs):
        yHat = model(data)

        loss = loss_func(yHat, labels)
        losses[epoch] = loss

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    prediction = model(data)
    accuracy = torch.mean(((prediction>0.5) == labels.bool()).float())*100

    return losses, accuracy
```

## Experiment Learning Rate vs Accuracy and Epochs vs Losses

```python
lrs = torch.linspace(0.001, 0.1, 40)

allLosses = torch.zeros((len(lrs), epochs))
accuracies = torch.zeros(len(lrs))

for i in range(len(lrs)):
    losses, accuracy = train(lrs[i])

    allLosses[i, :] = losses
    accuracies[i] = accuracy
```

```python
# Plot the Experiment Findings
_, axes = plt.subplots(1, 2, figsize=(12, 5))

# Learning Rate vs Accuracy
axes[0].plot(lrs.detach(), accuracies.detach(), marker='s', linestyle='-', markerfacecolor='w')
axes[0].set_xlabel('Learning Rate')
axes[0].set_ylabel('Accuracy')
axes[0].set_title(f'Accuracy(MAX={accuracies.max()}%, MIN={accuracies.min()}%) by Learning Rate')

# Epochs vs Losses
for i, lr in enumerate(lrs):
    axes[1].plot(range(len(allLosses[i])), allLosses[i].detach(), linestyle='-')
axes[1].set_xlabel('Loss')
axes[1].set_ylabel('Epochs')
axes[1].set_title('Losses by Learning Rate')

plt.show()
```

![png](7_ann_48_learning_rate_comparisons_files/7_ann_48_learning_rate_comparisons_13_0.png)

### Why model is either behaving very good or very poor?

It's really very difficult to say. There could be various possibility like

1. May be the model is not complex enough
2. May be the problem statement is a linear one and we used a non-linear model etc

Also it is very difficult to visualize whether the model is getting stuck in a local minima in case of poor performance. Because we are trying to minimize the loss function in 6(3 weights + 2 biases) + 1 (output feature) dimensions and it si not possible to visualize.

---

7_ann_49_multilayer_ann.md

# Reference

Section: 7 \
Lecture: 49 \
Title: Multilayer ANN \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842138 \
Udemy Reference Link: \
Pre-Requisite:

# Multilayer ANN

```python
import torch
from torch import nn
from matplotlib import pyplot as plt
```

```python
# Prepare Data
A = [ 1, 1 ]
B = [ 1, 5 ]
N = 100

a = torch.stack((A[0]+torch.randn(N), A[1]+torch.randn(N)), dim=1)
b = torch.stack((B[0]+torch.randn(N), B[1]+torch.randn(N)), dim=1)

data = torch.vstack((a, b))
labels = torch.vstack((torch.zeros(N, 1), torch.ones(N, 1)))
data.shape, labels.shape
```

    (torch.Size([200, 2]), torch.Size([200, 1]))

```python
# Visualize data
plt.scatter(data [torch.where(labels==0)[0], 0], data [torch.where(labels==0)[0], 1], marker='s', color='b', facecolor='w')
plt.scatter(data [torch.where(labels==1)[0], 0], data [torch.where(labels==1)[0], 1], marker='s', color='g', facecolor='w')
plt.show()
```

![png](7_ann_49_multilayer_ann_files/7_ann_49_multilayer_ann_4_0.png)

```python
# Build the model
class ANNMultiLayerBinaryClassifier(nn.Module):
    def __init__(self):
        super(ANNMultiLayerBinaryClassifier, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(2, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.ReLU(),
            nn.Linear(1, 1),
            nn.Sigmoid()
        )
    def forward(self, X):
        return self.stack(X)

ANNMultiLayerBinaryClassifier()
```

    ANNMultiLayerBinaryClassifier(
      (stack): Sequential(
        (0): Linear(in_features=2, out_features=16, bias=True)
        (1): ReLU()
        (2): Linear(in_features=16, out_features=1, bias=True)
        (3): ReLU()
        (4): Linear(in_features=1, out_features=1, bias=True)
        (5): Sigmoid()
      )
    )

```python
# Train the model
epochs = 1000

def train(lr):
    model = ANNMultiLayerBinaryClassifier()
    loss_func = nn.BCELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)

    losses = torch.zeros(epochs)
    for epoch in range(epochs):
        # forward
        yHat = model(data)

        # compute loss
        loss = loss_func(yHat, labels)
        losses[epoch] = loss

        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    prediction = model(data)
    accuracy = torch.mean(((prediction>0.5)==labels).float()) * 100

    return accuracy, losses
```

```python
# Test the code ones
accuracy, losses = train(lr=0.01)

plt.plot(range(epochs), losses.detach(), marker='o')
plt.title(f'Epoch vs Loss, accuracy={accuracy}')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```

![png](7_ann_49_multilayer_ann_files/7_ann_49_multilayer_ann_7_0.png)

## Experiment Learning Rate vs Accuracy and Epochs vs Losses

```python
lrs = torch.linspace(0.001, 0.1, 40)

allLosses = torch.zeros((len(lrs), epochs))
accuracies = torch.zeros(len(lrs))

for i in range(len(lrs)):
    accuracy, losses = train(lrs[i])

    allLosses[i, :] = losses
    accuracies[i] = accuracy
```

```python
# Plot the Experiment Findings
_, axes = plt.subplots(1, 2, figsize=(12, 5))

# Learning Rate vs Accuracy
axes[0].plot(lrs.detach(), accuracies.detach(), marker='s', linestyle='-', markerfacecolor='w')
axes[0].set_xlabel('Learning Rate')
axes[0].set_ylabel('Accuracy')
axes[0].set_title(f'Accuracy(MAX={accuracies.max()}%, MIN={accuracies.min()}%) by Learning Rate')

# Epochs vs Losses
for i, lr in enumerate(lrs):
    axes[1].plot(range(len(allLosses[i])), allLosses[i].detach(), linestyle='-')
axes[1].set_xlabel('Loss')
axes[1].set_ylabel('Epochs')
axes[1].set_title('Losses by Learning Rate')

plt.show()
```

![png](7_ann_49_multilayer_ann_files/7_ann_49_multilayer_ann_10_0.png)

### Why it is woking very good for certain lr and very poor for others?

I have observed that, for the same learning rate also, model sometimes performs good and sometimes performs bad. However, if I am increasing the number of epochs(1000, then 2000, then 5000), the frequency of getting bad accuracy is reduced. And the experiment is showing that in verity of learning rates. So, if we perform the experiment, once again, it is likely that the learning rate for which the accuracy was bad on nth experiment, may show good accuracy in (n+1)th attempt.
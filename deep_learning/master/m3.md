7_ann_50_linear_solutions_to_linear_problems.md

# Reference

Section: 7 \
Lecture: 50 \
Title: Linear solutions to linear problems \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842140 \
Udemy Reference Link: \
Pre-Requisite:

# Linear Solution to Linear Problems

## Demystifying the uncertainity to Qwerty's problem

The problem we faced earlier was with one perceptron or multilayer perceptron, for the exact same metaparameters, the model was either performing poorly or very good. Why is this the case?

It turns out that, Qwerty is a very simple binary classification problem. However to solve that, we are using non-linear model. So the model is trying to find a non-linear solution, whearase a simple linear line can solve the problem. So we can just remove the non-linearity(nn.ReLU activation fuctions) from the model and should give us consistent result.

```python
import torch
from torch import nn
from matplotlib import pyplot as plt
```

```python
# Prepare Data
A = [ 1, 1 ]
B = [ 1, 5 ]
N = 100

a = torch.stack((A[0]+torch.randn(N), A[1]+torch.randn(N)), dim=1)
b = torch.stack((B[0]+torch.randn(N), B[1]+torch.randn(N)), dim=1)

data = torch.vstack((a, b))
labels = torch.vstack((torch.zeros(N, 1), torch.ones(N, 1)))
data.shape, labels.shape
```

    (torch.Size([200, 2]), torch.Size([200, 1]))

```python
# Visualize data
plt.scatter(data [torch.where(labels==0)[0], 0], data [torch.where(labels==0)[0], 1], marker='s', color='b', facecolor='w')
plt.scatter(data [torch.where(labels==1)[0], 0], data [torch.where(labels==1)[0], 1], marker='s', color='g', facecolor='w')
plt.show()
```

![png](7_ann_50_linear_solutions_to_linear_problems_files/7_ann_50_linear_solutions_to_linear_problems_5_0.png)

```python
# Build the model
class ANNMultiLayerBinaryClassifier(nn.Module):
    def __init__(self):
        super(ANNMultiLayerBinaryClassifier, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(2, 16),
            # nn.ReLU(), # removed non-linearity
            nn.Linear(16, 1),
            # nn.ReLU(), # removed non-linearity
            nn.Linear(1, 1),
            nn.Sigmoid()
        )
    def forward(self, X):
        return self.stack(X)

ANNMultiLayerBinaryClassifier()
```

    ANNMultiLayerBinaryClassifier(
      (stack): Sequential(
        (0): Linear(in_features=2, out_features=16, bias=True)
        (1): Linear(in_features=16, out_features=1, bias=True)
        (2): Linear(in_features=1, out_features=1, bias=True)
        (3): Sigmoid()
      )
    )

```python
# Train the model
epochs = 1000

def train(lr):
    model = ANNMultiLayerBinaryClassifier()
    loss_func = nn.BCELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)

    losses = torch.zeros(epochs)
    for epoch in range(epochs):
        # forward
        yHat = model(data)

        # compute loss
        loss = loss_func(yHat, labels)
        losses[epoch] = loss

        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    prediction = model(data)
    accuracy = torch.mean(((prediction>0.5)==labels).float()) * 100

    return accuracy, losses
```

```python
# Test the code ones
accuracy, losses = train(lr=0.01)

plt.plot(range(epochs), losses.detach(), marker='o')
plt.title(f'Epoch vs Loss, accuracy={accuracy}')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```

![png](7_ann_50_linear_solutions_to_linear_problems_files/7_ann_50_linear_solutions_to_linear_problems_8_0.png)

## Experiment Learning Rate vs Accuracy and Epochs vs Losses

```python
lrs = torch.linspace(0.001, 0.1, 40)

allLosses = torch.zeros((len(lrs), epochs))
accuracies = torch.zeros(len(lrs))

for i in range(len(lrs)):
    accuracy, losses = train(lrs[i])

    allLosses[i, :] = losses
    accuracies[i] = accuracy
```

```python
# Plot the Experiment Findings
_, axes = plt.subplots(1, 2, figsize=(12, 5))

# Learning Rate vs Accuracy
axes[0].plot(lrs.detach(), accuracies.detach(), marker='s', linestyle='-', markerfacecolor='w')
axes[0].set_xlabel('Learning Rate')
axes[0].set_ylabel('Accuracy')
axes[0].set_title(f'Accuracy(MAX={accuracies.max()}%, MIN={accuracies.min()}%) by Learning Rate')

# Epochs vs Losses
for i, lr in enumerate(lrs):
    axes[1].plot(range(len(allLosses[i])), allLosses[i].detach(), linestyle='-')
axes[1].set_xlabel('Epochs')
axes[1].set_ylabel('Loss')
axes[1].set_title('Losses by Learning Rate')

plt.show()
```

![png](7_ann_50_linear_solutions_to_linear_problems_files/7_ann_50_linear_solutions_to_linear_problems_11_0.png)

```python

```

---

7*ann_52_multi-output_ANN*(iris_dataset).md

# Reference

Section: 7 \
Lecture: 52 \
Title: Multi-output ANN (iris dataset) \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842144 \
Udemy Reference Link: \
Pre-Requisite:

# Multi-output ANN (iris dataset)

## Why the Sigmoid Function is Inappropriate for Multiclass Classifier Models

In the case of a binary classification model, we use the sigmoid function because its output range is between 0 and 1. If the output value is less than 0.5, it is classified as Type 1; otherwise, it is classified as Type 2. However, for a multiclass classifier, the sigmoid function is not suitable. We need an activation function that gives the probabilities for each class, so that all the probabilities add up to 1. This is precisely what the softmax function accomplishes.

```python
import torch
from torch import nn

import matplotlib.pyplot as plt
import seaborn as sns
```

```python
# Load data
iris = sns.load_dataset('iris')
data = torch.tensor(iris [ iris.columns[:-1] ].values, dtype=torch.float32)
labels = torch.zeros(len(iris), dtype=torch.long)
# # labels[ iris['species'] == 'setosa'] = 0
labels[ iris['species'] == 'versicolor'] = 1
labels[ iris['species'] == 'virginica'] = 2

data.shape, labels.shape

class ANNMultiClassClassifier(nn.Module):
    def __init__(self):
        super(ANNMultiClassClassifier, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(4, 64),
            nn.ReLU(),
            nn.Linear(64,64),
            nn.ReLU(),
            nn.Linear(64, 3)
        )
    def forward(self, X):
        return self.stack(X)

model = ANNMultiClassClassifier()
model
```

```python
# train the model
loss_func= nn.CrossEntropyLoss()
optimizer= torch.optim.SGD(model.parameters(), lr=0.01)

epochs=1000
losses = torch.zeros(epochs)
accuracies = torch.zeros(epochs)

for epoch in range(epochs):
    # feed forward
    yHat = model(data)

    # claculate loss
    loss = loss_func(yHat, labels)
    losses[epoch] = loss
    accuracies[epoch] = torch.mean((yHat.argmax(dim=1) == labels).float())*100

    # backprop
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

```python
# Evaluate the model performance
prediction = model(data)
accuracy = torch.mean((prediction.argmax(dim=1) == labels).float())*100
accuracy
```

    tensor(98.)

```python
# plot the performance
_, axes = plt.subplots(1, 2, figsize=(12, 5))

# Epoch vs Loss
axes[0].plot(range(epochs), losses.detach(), marker='o', markerfacecolor='w')
axes[0].set_title(f'Epoch vs Loss MIN={torch.min(losses)}')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')

# Epoch vs Accuracy
axes[1].plot(range(epochs), accuracies.detach(), marker='o', color='g', markerfacecolor='w')
axes[1].set_title(f'Epoch vs Accuracy MAX={torch.max(accuracies)}')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')

plt.show()
```

![png](7_ann_52_multi-output_ANN_%28iris_dataset%29_files/7_ann_52_multi-output_ANN_%28iris_dataset%29_7_0.png)

## Question

### `BCEWithLogitsLoss` vs `CrossEntropyLoss`

#### BCEWithLogitsLoss

- **Purpose**: This loss function is used for binary classification problems. It combines a sigmoid layer and the binary cross-entropy loss in one single class. This is particularly useful when you have a single output neuron that predicts the probability of the positive class.
- **Input**: It expects raw logits (the output of the last layer before applying the sigmoid function) and target labels that are either 0 or 1.
- **Formula**: The loss is calculated as:
  $$
  text{loss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\sigma(x_i)) + (1 - y_i) \log(1 - \sigma(x_i))]
  $$
  where $( \sigma )$ is the sigmoid function.

#### CrossEntropyLoss

- **Purpose**: This loss function is used for multi-class classification problems. It expects the model's output to be a vector of raw logits for each class and the target labels to be class indices.
- **Input**: It takes raw logits (not probabilities) for multiple classes and target labels that are integers representing the class index.
- **Formula**: The loss is calculated as:
  $$
  text{loss} = -\frac{1}{N} \sum_{i=1}^{N} \log\left(\frac{e^{x_{i,y_i}}}{\sum_{j} e^{x_{i,j}}}\right)
  $$
  where $( x_{i,j})$ is the logit for class $( j )$ for the $( i )-th$ sample, and $( y_i )$ is the true class index for the $( i )-th$ sample.

### Why are we not using softmax in the Sequential itself?

In PyTorch, when using `nn.CrossEntropyLoss`, you do not need to apply a softmax activation function to the output of your model. This is because `nn.CrossEntropyLoss` combines both the **softmax** activation and the **negative log-likelihood** loss in a single function. This is similar to what `BCEWithLogitsLoss` which combines both `nn.Sigmoid` and `nn.BCELoss` loss in a single function.

### Why do labels have to be of long type, but not float32?

`nn.CrossEntropyLoss` is used for multi-class classification problem. Also it combines both the **softmax** activation and the **negative log-likelihood** loss in a single function.
As it is used for multi-class classification problem, it expects the model's output to be of shape `(N, C)` (where `N` is the batch size and `C` is the number of classes) and target to be of shape `(N,)`. Each label corresponds to a class, and the `CrossEntropyLoss` expects the target labels to be integers that indicate the class index (0, 1, 2, etc.).

### Labels.shape is 1D, model is returning (150,3), still CrossEntropy can calculate loss, how?

The labels for classification tasks in PyTorch should be of type `torch.long` (or `int64`) because they represent class indices. Each label corresponds to a class, and the `CrossEntropyLoss` expects the target labels to be integers that indicate the class index (0, 1, 2, etc.). Using `float32` would not be appropriate here, as it could lead to incorrect interpretations of the labels. The model's output is a set of logits for each class, and the loss function needs to compare these logits against integer class indices.

```python

```

---

7_ann_53_code_challenge_more_qwerties.md

# Reference

Section: 7 \
Lecture: 53 \
Title: CodeChallenge: more qwerties! \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842148 \
Udemy Reference Link: \
Pre-Requisite:

# Code Challenge more qwerties

```python
import torch
from torch import nn
import matplotlib.pyplot as plt
```

```python
# Prepare Data
N = 100
A = [ 1, 1 ]
B = [ 5, 1 ]
C = [ 3, -2 ]

a = torch.stack( ( A[0] + torch.randn(N), A[1] + torch.randn(N) ), dim=1 )
b = torch.stack( ( B[0] + torch.randn(N), B[1] + torch.randn(N) ), dim=1 )
c = torch.stack( ( C[0] + torch.randn(N), C[1] + torch.randn(N) ), dim=1 )

data = torch.vstack((a, b, c))
labels = torch.hstack( ( torch.zeros(N, dtype=torch.int64), torch.ones(N, dtype=torch.int64), torch.zeros(N, dtype=torch.int64)+2 ) )

data.shape, labels.shape
```

    (torch.Size([300, 2]), torch.Size([300]))

```python
# Visualize the data
plt.scatter(data[ torch.where(labels==0) ][:, 0], data[ torch.where(labels==0) ][:, 1], marker='s', color='r', facecolor='w')
plt.scatter(data[ torch.where(labels==1) ][:, 0], data[ torch.where(labels==1) ][:, 1], marker='s', color='g', facecolor='w')
plt.scatter(data[ torch.where(labels==2) ][:, 0], data[ torch.where(labels==2) ][:, 1], marker='s', color='b', facecolor='w')
plt.show()
```

![png](7_ann_53_code_challenge_more_qwerties_files/7_ann_53_code_challenge_more_qwerties_4_0.png)

```python
# Build the model
class ANNMultiClassQwerties(nn.Module):
    def __init__(self):
        super(ANNMultiClassQwerties, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(2, 4),
            nn.ReLU(),
            nn.Linear(4, 3),
            nn.Softmax(dim=1)
        )

    def forward(self, X):
        return self.stack(X)
ANNMultiClassQwerties()
```

    ANNMultiClassQwerties(
      (stack): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): ReLU()
        (2): Linear(in_features=4, out_features=3, bias=True)
        (3): Softmax(dim=1)
      )
    )

```python
# train the model
model = ANNMultiClassQwerties()
epochs = 10000
loss_func = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

losses = torch.zeros(epochs)
accuracies = torch.zeros(epochs)

for epoch in range(epochs):
    # feed forward
    yHat = model(data)

    # calculate loss
    loss = loss_func(yHat, labels)
    losses[epoch] = loss
    accuracies[epoch] = torch.mean( (yHat.argmax(dim=1) == labels).float() ) * 100

    # backprop
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

```python
# plot the model performance

_, axes = plt.subplots(1, 2, figsize=(12, 5))

# epoch vs loss
axes[0].plot(range(epochs), losses.detach(), marker='o', markerfacecolor='w')
axes[0].set_title(f'Epochs vs Loss MIN={losses.min()}')
axes[0].set_xlabel('Epochs')
axes[0].set_ylabel('Loss')

axes[1].plot(range(epochs), accuracies.detach(), marker='o', color='g', markerfacecolor='w')
axes[1].set_title(f'Epochs vs Accuracy MAX={accuracies.max()}')
axes[1].set_xlabel('Epochs')
axes[1].set_ylabel('Accuracy')

plt.show()
```

![png](7_ann_53_code_challenge_more_qwerties_files/7_ann_53_code_challenge_more_qwerties_7_0.png)

## Question

CrossEntropyLoss computes log-softmax internally. Does that mean that the Softmax() layer in the model needs to be there? Does it hurt or help? If you remove that final layer, what would change and what would be the same in the rest of the notebook?

---

7_ann_54_comparing_the_number_of_hidden_units.md

# Reference

Section: 7 \
Lecture: 54 \
Title: Comparing the number of hidden units \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842152 \
Udemy Reference Link: \
Pre-Requisite:

```python
import torch
from torch import nn

import seaborn as sns

import matplotlib.pyplot as plt
```

```python
iris = sns.load_dataset('iris')

data = torch.tensor( iris[ iris.columns[:-1] ].values, dtype=torch.float32 )

labels = torch.zeros(len(iris), dtype=torch.int64)
# labels[ iris['species'] == 'versicolor' ] = 0
labels[ iris['species'] == 'versicolor' ] = 1
labels[ iris['species'] == 'virginica' ] = 2

data.shape, labels.shape
```

    (torch.Size([150, 4]), torch.Size([150]))

```python
# build model
class ANNIrisClassifier(nn.Module):
    def __init__(self, n_units):
        super(ANNIrisClassifier, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(4, n_units),
            nn.ReLU(),
            nn.Linear(n_units, 3)
        )

    def forward(self, X):
        return self.stack(X)

ANNIrisClassifier(5)
```

    ANNIrisClassifier(
      (stack): Sequential(
        (0): Linear(in_features=4, out_features=5, bias=True)
        (1): ReLU()
        (2): Linear(in_features=5, out_features=3, bias=True)
      )
    )

```python
# train the model
epochs = 150

def train_model(n_units):
    losses, accuracies = torch.zeros(epochs), torch.zeros(epochs)

    model = ANNIrisClassifier(n_units)
    loss_func = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    for epoch in range(epochs):
        # forward pass
        yHat = model(data)

        # evaluate loss
        loss = loss_func(yHat, labels)
        losses[epoch] = loss
        accuracies[epoch] = torch.mean( (yHat.argmax(dim=1) == labels).float() )*100

        # backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return losses, accuracies
```

```python
# test the code once and plot the result
losses, accuracies = train_model(16)

_, axes = plt.subplots(1, 2, figsize=(12, 5))

# Epoch vs Loss
axes[0].plot(range(epochs), losses.detach(), marker='o', markerfacecolor='w')
axes[0].set_title(f'Epoch vs Loss MIN={torch.min(losses)}')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')

# Epoch vs Accuracy
axes[1].plot(range(epochs), accuracies.detach(), marker='o', color='g', markerfacecolor='w')
axes[1].set_title(f'Epoch vs Accuracy MAX={torch.max(accuracies)}')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')

plt.show()
```

![png](7_ann_54_comparing_the_number_of_hidden_units_files/7_ann_54_comparing_the_number_of_hidden_units_5_0.png)

```python
# Parametric Experiment
n_unit_range = torch.arange(1, 129)

max_accuracies = torch.zeros(len(n_unit_range))

for n_units in range(len(n_unit_range)):
    _ , accuracies = train_model(n_units)
    max_accuracies[n_units] = accuracies.max()
```

```python
# Epoch vs Accuracy
plt.plot(n_unit_range.detach(), max_accuracies.detach(), marker='o', color='g', markerfacecolor='w')
plt.title(f'Hidden Units vs Accuracy MAX={torch.max(max_accuracies)}')
plt.xlabel('Hidden Units')
plt.ylabel('Max Accuracy')
plt.show()
```

![png](7_ann_54_comparing_the_number_of_hidden_units_files/7_ann_54_comparing_the_number_of_hidden_units_7_0.png)

```python

```

---

7_ann_55_depth_vs_breadth_number_of_parameters.md

# Reference

Section: 7 \
Lecture: 55 \
Title: Depth vs. breadth: number of parameters \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842154 \
Udemy Reference Link: \
Pre-Requisite:

# Depth vs Breadth Number Of Parameters

## Breadth vs Depth of ANN

Depth is the number of hidden layers (layers between input and output)

Breadth/width is the number of units per hidden layer(can vary across layers)

```python
import torch.nn as nn
from torchsummary import summary
```

```python
class ANNWide(nn.Module):
    def __init__(self):
        super(ANNWide, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(2, 4),
            nn.Linear(4, 3)
        )
    def forward(self, X):
        return self.stack(X)

ANNWide()
```

    ANNWide(
      (stack): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): Linear(in_features=4, out_features=3, bias=True)
      )
    )

```python
class ANNDeep(nn.Module):
    def __init__(self):
        super(ANNDeep, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(2, 2),
            nn.Linear(2, 2),
            nn.Linear(2, 3)
        )
    def forward(self, X):
        return self.stack(X)

ANNDeep()
```

    ANNDeep(
      (stack): Sequential(
        (0): Linear(in_features=2, out_features=2, bias=True)
        (1): Linear(in_features=2, out_features=2, bias=True)
        (2): Linear(in_features=2, out_features=3, bias=True)
      )
    )

### Counting number of Nodes/Units in ANN [`.named_parameters()`]

```python
# named_parameters() is an iterable that returns the tuple (name,numbers)

n_ann_wide_nodes = 0
for param_name, param_weights in ANNWide().named_parameters():
    # print(param_name, end='\n\n')
    # print(param_tensor, end='\n\n')
    if 'bias' in param_name:
        n_ann_wide_nodes += len(param_weights)

n_ann_deep_nodes = 0
for param_name, param_weights in ANNDeep().named_parameters():
    if 'bias' in param_name:
        n_ann_deep_nodes += len(param_weights)


# In both the cases, we are not considering the nodes in input layer
print(f'Number of nodes in ANNWide is {n_ann_wide_nodes}')
print(f'Number of nodes in ANNDeep is {n_ann_deep_nodes}')
```

    Number of nodes in ANNWide is 7
    Number of nodes in ANNDeep is 7

### Counting number of Trainable Parameters in ANN [`.numel()`, `.parameters()`]

```python
# Note, we pass ANNWide().parameters in optimizer constructor
n_ann_wide_trained_param = 0
for param in ANNWide().parameters():
    # print(param)
    # print(param.requires_grad)
    n_ann_wide_trained_param += param.numel()

n_ann_deep_trained_param = 0
for param in ANNDeep().parameters():
    # print(param)
    # print(param.requires_grad)
    n_ann_deep_trained_param += param.numel()

print(f'Number of trainable parameters in ANNWide is {n_ann_wide_trained_param}')
print(f'Number of trainable parameters in ANNDeep is {n_ann_deep_trained_param}')
```

    Number of trainable parameters in ANNWide is 27
    Number of trainable parameters in ANNDeep is 21

```python
# summary(ANNWide, (1,2))
# this is supposed to work, but not sure why it is not working
```

---

7_ann_56_defining_models_using_sequential_vs_class.md

# Reference

Section: 7 \
Lecture: 56 \
Title: Defining models using sequential vs. class \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842158 \
Udemy Reference Link: \
Pre-Requisite:

# Defining Models using Sequential vs Class

`nn.Sequential` is pretty easy and quick to create. However it is pretty limited in terms of creating model architecture. However using a class, we can create complex model architecture and the using the class approach we can perform any level of customization we want.

`nn.ModuleDict` is used

```python
import torch
import torch.nn as nn
from torch.nn import functional as F
```

## Building ANN using just nn.Sequential()

```python
ANNSequential = nn.Sequential(
    nn.Linear(2, 4),
    nn.ReLU(),
    nn.Linear(4, 3)
)

ANNSequential(torch.randn(10, 2))
```

    tensor([[-0.2822,  0.3955, -0.2888],
            [-0.4650,  0.2975, -0.1183],
            [-0.0393,  0.4018, -0.3267],
            [-0.4650,  0.2975, -0.1183],
            [ 0.0518,  0.4587, -0.4060],
            [ 0.1414,  0.4263, -0.3784],
            [-0.0868,  0.4884, -0.4243],
            [-0.1864,  0.4833, -0.4058],
            [-0.4650,  0.2975, -0.1183],
            [-0.3524,  0.4256, -0.3161]], grad_fn=<AddmmBackward0>)

## Building ANN without nn.Sequential() in a class

```python
class ANNClass(nn.Module):
    def __init__(self):
        super(ANNClass, self).__init__()
        self.l01 = nn.Linear(2, 4)
        self.l12 = nn.Linear(4, 3)

    def forward(self, X):
        Y = self.l01(X)
        Y = nn.ReLU(Y)
        Y = self.l12(Y)
        return Y

ANNClass()
```

    ANNClass(
      (l01): Linear(in_features=2, out_features=4, bias=True)
      (l12): Linear(in_features=4, out_features=3, bias=True)
    )

## Building ANN using nn.Sequential() in a class

```python
class ANNSequentialClass(nn.Module):
    def __init__(self):
        super(ANNSequentialClass, self).__init__()
        self.stack = nn.Sequential(
            nn.Linear(2, 4),
            nn.ReLU(),
            nn.Linear(4, 3)
        )

    def forward(self, X):
        return self.stack(X)

ANNSequentialClass()
```

    ANNSequentialClass(
      (stack): Sequential(
        (0): Linear(in_features=2, out_features=4, bias=True)
        (1): ReLU()
        (2): Linear(in_features=4, out_features=3, bias=True)
      )
    )

## Building ANN using nn.ModuleDict()

```python
class ANNModuleDict(nn.Module):
    def __init__(self, n_layers, n_units_per_layer):
        super(ANNModuleDict, self).__init__()

        self.n_hidden_layers = n_layers
        self.stack = nn.ModuleDict()

        # Input => Hidden 0
        self.stack['ih1'] = nn.Linear(4, n_units_per_layer)

        # Building Hidden Layers
        for layer_no in range(1, n_layers):
            self.stack[f'h{layer_no}h{layer_no+1}'] = nn.Linear(n_units_per_layer, n_units_per_layer)

        # Hidden n => output
        self.stack[f'h{self.n_hidden_layers}o'] = nn.Linear(n_units_per_layer, 3)

    def forward(self, X):
        Y = self.stack['ih1'](X)

        for layer_no in range(1, self.n_hidden_layers):
            Y = F.relu( self.stack[f'h{layer_no}h{layer_no+1}'](Y) )

        Y = self.stack[f'h{self.n_hidden_layers}o'](Y)

        return Y

ANNModuleDict(n_layers=3, n_units_per_layer=10)
```

    ANNModuleDict(
      (stack): ModuleDict(
        (ih1): Linear(in_features=4, out_features=10, bias=True)
        (h1h2): Linear(in_features=10, out_features=10, bias=True)
        (h2h3): Linear(in_features=10, out_features=10, bias=True)
        (h3o): Linear(in_features=10, out_features=3, bias=True)
      )
    )

```python

```

---

7_ann_57_model_depth_vs_breadth.md

# Reference

Section: 7 \
Lecture: 57 \
Title: Model depth vs. breadth \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842160 \
Udemy Reference Link: \
Pre-Requisite:

# Model depth vs breadth

```python
import torch
from torch import nn
from torch.nn import functional as F

import numpy as np

import seaborn as sns
from matplotlib import pyplot as plt
```

```python
# Load data
iris = sns.load_dataset('iris')
data = torch.tensor(iris [ iris.columns[:-1] ].values, dtype=torch.float32)
labels = torch.zeros(len(iris), dtype=torch.long)
# # labels[ iris['species'] == 'setosa'] = 0
labels[ iris['species'] == 'versicolor'] = 1
labels[ iris['species'] == 'virginica'] = 2

data.shape, labels.shape
```

    (torch.Size([150, 4]), torch.Size([150]))

```python
# Build the model
class ANNIris(nn.Module):
    def __init__(self, n_layers, n_units_per_layer):
        super(ANNIris, self).__init__()

        self.n_hidden_layers = n_layers
        self.stack = nn.ModuleDict()

        # Input => Hidden 0
        self.stack['ih1'] = nn.Linear(4, n_units_per_layer)

        # Building Hidden Layers
        for layer_no in range(1, n_layers):
            self.stack[f'h{layer_no}h{layer_no+1}'] = nn.Linear(n_units_per_layer, n_units_per_layer)

        # Hidden n => output
        self.stack[f'h{self.n_hidden_layers}o'] = nn.Linear(n_units_per_layer, 3)

    def forward(self, X):
        Y = self.stack['ih1'](X)

        for layer_no in range(1, self.n_hidden_layers):
            Y = F.relu( self.stack[f'h{layer_no}h{layer_no+1}'](Y) )

        Y = self.stack[f'h{self.n_hidden_layers}o'](Y)

        return Y

ANNIris(n_layers=4, n_units_per_layer=12)
```

    ANNIris(
      (stack): ModuleDict(
        (ih1): Linear(in_features=4, out_features=12, bias=True)
        (h1h2): Linear(in_features=12, out_features=12, bias=True)
        (h2h3): Linear(in_features=12, out_features=12, bias=True)
        (h3h4): Linear(in_features=12, out_features=12, bias=True)
        (h4o): Linear(in_features=12, out_features=3, bias=True)
      )
    )

```python
# A quick test of running some numbers through the model.
# This simply ensures that the architecture is internally consistent.

# 10 samples, 4 dimensions
tmpx = torch.randn(10,4)

# run it through the DL
y = ANNIris(n_layers=4, n_units_per_layer=12)(tmpx)

# exam the shape of the output
print( y.shape ), print(' ')

# and the output itself
print(y)
```

    torch.Size([10, 3])

    tensor([[-0.0290, -0.1348, -0.3418],
            [-0.0212, -0.1566, -0.3328],
            [-0.0133, -0.1377, -0.3232],
            [-0.0182, -0.1663, -0.3292],
            [-0.0145, -0.1673, -0.3272],
            [-0.0162, -0.1685, -0.3233],
            [-0.0115, -0.1704, -0.3232],
            [ 0.0047, -0.1331, -0.3053],
            [-0.0078, -0.1660, -0.3306],
            [-0.0262, -0.1481, -0.3279]], grad_fn=<AddmmBackward0>)

```python
# train the model
epochs=1000

def train_the_model(model):
    loss_func= nn.CrossEntropyLoss()
    optimizer= torch.optim.SGD(model.parameters(), lr=0.01)

    losses = torch.zeros(epochs)
    accuracies = torch.zeros(epochs)

    for epoch in range(epochs):
        # feed forward
        yHat = model(data)

        # claculate loss
        loss = loss_func(yHat, labels)
        losses[epoch] = loss
        accuracies[epoch] = torch.mean((yHat.argmax(dim=1) == labels).float())*100

        # backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    accuracy = torch.mean( ( torch.argmax( prediction, dim=1 ) == labels ).float() )*100
    n_trainable_params = sum(param.numel() for param in model.parameters())

    return accuracy, n_trainable_params
```

```python
model = ANNIris(n_layers=4, n_units_per_layer=12)
accuracy, n_trainable_params = train_the_model(model)
```

## Parametric Experiment

```python
# define the model parameters
numlayers = range(1,6)         # number of hidden layers
numunits  = np.arange(4,101,3) # units per hidden layer

# initialize output matrices
accuracies  = np.zeros((len(numunits),len(numlayers)))
totalparams = np.zeros((len(numunits),len(numlayers)))

# number of training epochs
numepochs = 500


# start the experiment!
for unitidx in range(len(numunits)):
  for layeridx in range(len(numlayers)):

    # create a fresh model instance
    net = ANNIris(numunits[unitidx],numlayers[layeridx])

    # run the model and store the results
    acc,nParams = train_the_model(net)
    accuracies[unitidx,layeridx] = acc

    # store the total number of parameters in the model
    totalparams[unitidx,layeridx] = nParams
```

```python
# show accuracy as a function of model depth
fig,ax = plt.subplots(1,figsize=(12,6))

ax.plot(numunits,accuracies,'o-',markerfacecolor='w',markersize=9)
ax.plot(numunits[[0,-1]],[33,33],'--',color=[.8,.8,.8])
ax.plot(numunits[[0,-1]],[67,67],'--',color=[.8,.8,.8])
ax.legend(numlayers)
ax.set_ylabel('accuracy')
ax.set_xlabel('Number of hidden units')
ax.set_title('Accuracy')
plt.show()
```

![png](7_ann_57_model_depth_vs_breadth_files/7_ann_57_model_depth_vs_breadth_10_0.png)

```python
# Maybe it's simply a matter of more parameters -> better performance?

# vectorize for convenience
x = totalparams.flatten()
y = accuracies.flatten()

# correlation between them
r = np.corrcoef(x,y)[0,1]

# scatter plot
plt.plot(x,y,'o')
plt.xlabel('Number of parameters')
plt.ylabel('Accuracy')
plt.title('Correlation: r=' + str(np.round(r,3)))
plt.show()
```

    C:\my_learning\python\ai\.venv\Lib\site-packages\numpy\lib\_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide
      c /= stddev[:, None]
    C:\my_learning\python\ai\.venv\Lib\site-packages\numpy\lib\_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide
      c /= stddev[None, :]

![png](7_ann_57_model_depth_vs_breadth_files/7_ann_57_model_depth_vs_breadth_11_1.png)

## Learning

It is not necessary that more number of layers means more better performance

It is not nacessary that more number of units per later means more better performance

Also it is not necessary that the more the number of trainable parameters means the better the performance

The model gets complecated very quickly when we have a need to tune various metaparameters.

---

7_ann_60_reflection_are_dl_models_understandable_yet.md

# Reference

Section: 7 \
Lecture: 60 \
Title: Reflection: Are DL models understandable yet? \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842168 \
Udemy Reference Link: \
Pre-Requisite:

# Reflection: Are DL models understandable yet?

- Each node of a deep learning model os performing a very simple mathematical operations.
- Yet their interactions and non-linearities create a level of complexity that makes it difficult to understand what each unit in the model is doing.
- Research is actively being conducted to improve our understanding of how deep learning models represent inputs and make decisions.
- There are differing views on whether we truly understand how deep learning works, with some believing the equations are clear and others arguing that the models operate in ways that are not fully comprehensible.

---

8_overfitting_and_cross_validation_61_what_is_overfitting_and_is_it_as_bad_as_they_say.md

# Reference

Section: 8 \
Lecture: 61 \
Title: What is overfitting and is it as bad as they say? \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27844168 \
Udemy Reference Link: \
Pre-Requisite:

# What is overfitting and is it as bad as they say?

```python
from IPython.display import Image, display
```

## Problem of overfitting and underfitting

Higher parameter model is not necessarily always better than low parameter model.
In the below diagram, the 10 parameter model fits the data more accurately than the 2 parameter model. But with 10 parameter model we see the model is getting overfitted. But turns out that, the 10 parameter model is performing very bad in Dataset 2.

```python
display(Image(filename='overfitting_61_03_23.png', width=500, height=400))
```

![png](8_overfitting_and_cross_validation_61_what_is_overfitting_and_is_it_as_bad_as_they_say_files/8_overfitting_and_cross_validation_61_what_is_overfitting_and_is_it_as_bad_as_they_say_4_0.png)

```python
display(Image(filename='underfitting_63_03_33.png', width=500, height=400))
```

![png](8_overfitting_and_cross_validation_61_what_is_overfitting_and_is_it_as_bad_as_they_say_files/8_overfitting_and_cross_validation_61_what_is_overfitting_and_is_it_as_bad_as_they_say_5_0.png)

| Overfitting                                            | Underfitting                        |
| ------------------------------------------------------ | ----------------------------------- |
| Overly sensitive to noise                              | **Less sensitive to noise**         |
| Increased sensitivity to subtle effects                | Less likely to detect true effects  |
| Reduced generalizability                               | Reduced generalizability            |
| Over-parameterized models become difficult to estimate | **Parameters are better estimated** |
|                                                        | **Good results with less data**     |

generalizability means, the ability of the model to predict on new data, data that the model has not seen before.

## Researcher Overfitting / Researcher Degrees of Freedom

Researcher degrees of freedom refers to the various choices that a researcher or data analyst has when it comes to cleaning, organizing, and selecting data, as well as the decisions made in model creation, including which parameters and architectures to use. This flexibility can lead to issues, as fine-tuning a model for a specific dataset may hinder its ability to generalize to new datasets.

### Example:

Imagine you are analyzing a dataset and decide to experiment with three different deep learning model architectures, which we’ll label as Model A, Model B, and Model C. After running these models, you find their performance unsatisfactory.

To improve the results, you revisit the dataset and apply a different set of criteria for data cleaning and selection. After reprocessing the data, you test the three models again. This time, Model B emerges as the top performer, achieving the highest accuracy.

Excited by these results, you publish your findings in a scientific journal, share the model on GitHub, or write a blog post about it. However, because you selected and tested these models on two different versions of the same dataset, you risk overfitting the entire model space to this specific scenario.

As a result, you cannot confidently assert that Model B will perform equally well on a different dataset. The concept of researcher degrees of freedom highlights that the more decisions you make based on a particular dataset, the less likely it is that your chosen model will be effective when applied to new or different data.

### How to Avoid Researcher Overfitting

To mitigate the risk of researcher overfitting, consider the following strategies:

1. **Predefine Model Architecture**:

   - Choose the model architecture in advance and make only minor adjustments as needed.
   - This approach is commonly used in traditional statistics and machine learning, where researchers have established effective models over time for frequently studied problems.
   - For example, in image recognition tasks, starting with a well-known architecture like ResNet and applying transfer learning can help avoid overfitting. Transfer learning leverages pre-trained models, allowing for better generalization.

2. **Reserve a Test Set**:
   - Set aside a portion of the data as a test set that is not used during the model training process.
   - Build and train your models (e.g., Model A, B, and C) using the training data, but keep the test data completely separate.
   - Only evaluate the models on the test data after all training and parameter selection is complete. This ensures that the model's performance is assessed on unseen data, providing a more accurate measure of its generalizability.

```python

```

---

todo_7_ann_51_why_multilayer_linear_models_don't_exist.md

# Reference

Section: 7 \
Lecture: 51 \
Title: Why multilayer linear models don't exist \
TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842142 \
Udemy Reference Link: \
Pre-Requisite:

# Why multilayer linear models don't exist

**Without non-linear activations, multiple "layers" collapse into a single linear transformation.**
Any sequence of linear functions can be combined into one equivalent linear function by multiplying their weight matrices.

**Non-linearities are essential between layers.**  
 You must include some form of non-linearity—such as ReLU, tanh, sigmoid, pooling, etc.—to give depth to your model’s representational power.

### Example (with non-linearity)

- Layer 1 output: $\hat y_1 = \sigma(W_1^T . x )$
- Layer 2 output: $\hat y_2 = \sigma(W_2^T . x )$
  - $\sigma$ can be any non-linear activation (ReLU, tanh, etc.)

### What Happens If $\sigma$ is Linear or Omitted

- If $\sigma$ is the identity (a linear function), it can be absorbed into the weight matrices.
- Layer 2 becomes:
  $$
  \hat y_2 = W_2^T \bigl(W_1^T x\bigr)
           = (W_1 W_2)^T x
           = W_{\alpha}^T x
  $$
- All subsequent layers similarly collapse: effectively one layer with weights.

### Numerical Illustration

- Non-linear example: $log_{10}( 5 + 5 ) = 1$ but $log_{10}( 5 + 5 ) \neq 1$ \
  Demonstrates that non-linear functions do not distribute over addition.
- Linear example: $A.( 5 + 5 ) = A . 10$ and $ A.5 + A.5 = A.10 $ \
  Shows linear functions distribute, allowing collapse into a single weighted sum.
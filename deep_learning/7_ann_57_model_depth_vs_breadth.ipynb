{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45adf9c3",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Section: 7 \\\n",
    "Lecture: 57 \\\n",
    "Title: Model depth vs. breadth \\\n",
    "TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842160 \\\n",
    "Udemy Reference Link: \\\n",
    "Pre-Requisite:\n",
    "\n",
    "# Model depth vs breadth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfe63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f70240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "iris = sns.load_dataset('iris')\n",
    "data = torch.tensor(iris [ iris.columns[:-1] ].values, dtype=torch.float32)\n",
    "labels = torch.zeros(len(iris), dtype=torch.long)\n",
    "# # labels[ iris['species'] == 'setosa'] = 0\n",
    "labels[ iris['species'] == 'versicolor'] = 1\n",
    "labels[ iris['species'] == 'virginica'] = 2\n",
    "\n",
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cd61c",
   "metadata": {},
   "source": [
    "    (torch.Size([150, 4]), torch.Size([150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d32509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "class ANNIris(nn.Module):\n",
    "    def __init__(self, n_layers, n_units_per_layer):\n",
    "        super(ANNIris, self).__init__()\n",
    "\n",
    "        self.n_hidden_layers = n_layers\n",
    "        self.stack = nn.ModuleDict()\n",
    "\n",
    "        # Input => Hidden 0\n",
    "        self.stack['ih1'] = nn.Linear(4, n_units_per_layer)\n",
    "\n",
    "        # Building Hidden Layers\n",
    "        for layer_no in range(1, n_layers):\n",
    "            self.stack[f'h{layer_no}h{layer_no+1}'] = nn.Linear(n_units_per_layer, n_units_per_layer)\n",
    "\n",
    "        # Hidden n => output\n",
    "        self.stack[f'h{self.n_hidden_layers}o'] = nn.Linear(n_units_per_layer, 3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.stack['ih1'](X)\n",
    "\n",
    "        for layer_no in range(1, self.n_hidden_layers):\n",
    "            Y = F.relu( self.stack[f'h{layer_no}h{layer_no+1}'](Y) )\n",
    "\n",
    "        Y = self.stack[f'h{self.n_hidden_layers}o'](Y)\n",
    "\n",
    "        return Y\n",
    "\n",
    "ANNIris(n_layers=4, n_units_per_layer=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a9f2a",
   "metadata": {},
   "source": [
    "    ANNIris(\n",
    "      (stack): ModuleDict(\n",
    "        (ih1): Linear(in_features=4, out_features=12, bias=True)\n",
    "        (h1h2): Linear(in_features=12, out_features=12, bias=True)\n",
    "        (h2h3): Linear(in_features=12, out_features=12, bias=True)\n",
    "        (h3h4): Linear(in_features=12, out_features=12, bias=True)\n",
    "        (h4o): Linear(in_features=12, out_features=3, bias=True)\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3628fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick test of running some numbers through the model.\n",
    "# This simply ensures that the architecture is internally consistent.\n",
    "\n",
    "# 10 samples, 4 dimensions\n",
    "tmpx = torch.randn(10,4)\n",
    "\n",
    "# run it through the DL\n",
    "y = ANNIris(n_layers=4, n_units_per_layer=12)(tmpx)\n",
    "\n",
    "# exam the shape of the output\n",
    "print( y.shape ), print(' ')\n",
    "\n",
    "# and the output itself\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979cf9b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "    torch.Size([10, 3])\n",
    "\n",
    "    tensor([[-0.0290, -0.1348, -0.3418],\n",
    "            [-0.0212, -0.1566, -0.3328],\n",
    "            [-0.0133, -0.1377, -0.3232],\n",
    "            [-0.0182, -0.1663, -0.3292],\n",
    "            [-0.0145, -0.1673, -0.3272],\n",
    "            [-0.0162, -0.1685, -0.3233],\n",
    "            [-0.0115, -0.1704, -0.3232],\n",
    "            [ 0.0047, -0.1331, -0.3053],\n",
    "            [-0.0078, -0.1660, -0.3306],\n",
    "            [-0.0262, -0.1481, -0.3279]], grad_fn=<AddmmBackward0>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ecc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs=1000\n",
    "\n",
    "def train_the_model(model):\n",
    "    loss_func= nn.CrossEntropyLoss()\n",
    "    optimizer= torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    losses = torch.zeros(epochs)\n",
    "    accuracies = torch.zeros(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # feed forward\n",
    "        yHat = model(data)\n",
    "\n",
    "        # claculate loss\n",
    "        loss = loss_func(yHat, labels)\n",
    "        losses[epoch] = loss\n",
    "        accuracies[epoch] = torch.mean((yHat.argmax(dim=1) == labels).float())*100\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy = torch.mean( ( torch.argmax( prediction, dim=1 ) == labels ).float() )*100\n",
    "    n_trainable_params = sum(param.numel() for param in model.parameters())\n",
    "\n",
    "    return accuracy, n_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANNIris(n_layers=4, n_units_per_layer=12)\n",
    "accuracy, n_trainable_params = train_the_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463cbba",
   "metadata": {},
   "source": [
    "## Parametric Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88173d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model parameters\n",
    "numlayers = range(1,6)         # number of hidden layers\n",
    "numunits  = np.arange(4,101,3) # units per hidden layer\n",
    "\n",
    "# initialize output matrices\n",
    "accuracies  = np.zeros((len(numunits),len(numlayers)))\n",
    "totalparams = np.zeros((len(numunits),len(numlayers)))\n",
    "\n",
    "# number of training epochs\n",
    "numepochs = 500\n",
    "\n",
    "\n",
    "# start the experiment!\n",
    "for unitidx in range(len(numunits)):\n",
    "  for layeridx in range(len(numlayers)):\n",
    "\n",
    "    # create a fresh model instance\n",
    "    net = ANNIris(numunits[unitidx],numlayers[layeridx])\n",
    "\n",
    "    # run the model and store the results\n",
    "    acc,nParams = train_the_model(net)\n",
    "    accuracies[unitidx,layeridx] = acc\n",
    "\n",
    "    # store the total number of parameters in the model\n",
    "    totalparams[unitidx,layeridx] = nParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show accuracy as a function of model depth\n",
    "fig,ax = plt.subplots(1,figsize=(12,6))\n",
    "\n",
    "ax.plot(numunits,accuracies,'o-',markerfacecolor='w',markersize=9)\n",
    "ax.plot(numunits[[0,-1]],[33,33],'--',color=[.8,.8,.8])\n",
    "ax.plot(numunits[[0,-1]],[67,67],'--',color=[.8,.8,.8])\n",
    "ax.legend(numlayers)\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_xlabel('Number of hidden units')\n",
    "ax.set_title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1719a74",
   "metadata": {},
   "source": [
    "![png](7_ann_57_model_depth_vs_breadth_files/7_ann_57_model_depth_vs_breadth_10_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f40a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe it's simply a matter of more parameters -> better performance?\n",
    "\n",
    "# vectorize for convenience\n",
    "x = totalparams.flatten()\n",
    "y = accuracies.flatten()\n",
    "\n",
    "# correlation between them\n",
    "r = np.corrcoef(x,y)[0,1]\n",
    "\n",
    "# scatter plot\n",
    "plt.plot(x,y,'o')\n",
    "plt.xlabel('Number of parameters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Correlation: r=' + str(np.round(r,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d332535",
   "metadata": {},
   "source": [
    "    C:\\my_learning\\python\\ai\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n",
    "      c /= stddev[:, None]\n",
    "    C:\\my_learning\\python\\ai\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide\n",
    "      c /= stddev[None, :]\n",
    "\n",
    "![png](7_ann_57_model_depth_vs_breadth_files/7_ann_57_model_depth_vs_breadth_11_1.png)\n",
    "\n",
    "## Learning\n",
    "\n",
    "It is not necessary that more number of layers means more better performance\n",
    "\n",
    "It is not nacessary that more number of units per later means more better performance\n",
    "\n",
    "Also it is not necessary that the more the number of trainable parameters means the better the performance\n",
    "\n",
    "The model gets complecated very quickly when we have a need to tune various metaparameters."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

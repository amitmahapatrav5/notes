{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60170738",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Section: 7 \\\n",
    "Lecture: 53 \\\n",
    "Title: CodeChallenge: more qwerties! \\\n",
    "TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842148 \\\n",
    "Udemy Reference Link: \\\n",
    "Pre-Requisite:\n",
    "\n",
    "# Code Challenge more qwerties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b67321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec17b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "N = 100\n",
    "A = [ 1, 1 ]\n",
    "B = [ 5, 1 ]\n",
    "C = [ 3, -2 ]\n",
    "\n",
    "a = torch.stack( ( A[0] + torch.randn(N), A[1] + torch.randn(N) ), dim=1 )\n",
    "b = torch.stack( ( B[0] + torch.randn(N), B[1] + torch.randn(N) ), dim=1 )\n",
    "c = torch.stack( ( C[0] + torch.randn(N), C[1] + torch.randn(N) ), dim=1 )\n",
    "\n",
    "data = torch.vstack((a, b, c))\n",
    "labels = torch.hstack( ( torch.zeros(N, dtype=torch.int64), torch.ones(N, dtype=torch.int64), torch.zeros(N, dtype=torch.int64)+2 ) )\n",
    "\n",
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416656bf",
   "metadata": {},
   "source": [
    "    (torch.Size([300, 2]), torch.Size([300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37924905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.scatter(data[ torch.where(labels==0) ][:, 0], data[ torch.where(labels==0) ][:, 1], marker='s', color='r', facecolor='w')\n",
    "plt.scatter(data[ torch.where(labels==1) ][:, 0], data[ torch.where(labels==1) ][:, 1], marker='s', color='g', facecolor='w')\n",
    "plt.scatter(data[ torch.where(labels==2) ][:, 0], data[ torch.where(labels==2) ][:, 1], marker='s', color='b', facecolor='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ad3d",
   "metadata": {},
   "source": [
    "![png](7_ann_53_code_challenge_more_qwerties_files/7_ann_53_code_challenge_more_qwerties_4_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead229f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "class ANNMultiClassQwerties(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANNMultiClassQwerties, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.stack(X)\n",
    "ANNMultiClassQwerties()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39c288",
   "metadata": {},
   "source": [
    "    ANNMultiClassQwerties(\n",
    "      (stack): Sequential(\n",
    "        (0): Linear(in_features=2, out_features=4, bias=True)\n",
    "        (1): ReLU()\n",
    "        (2): Linear(in_features=4, out_features=3, bias=True)\n",
    "        (3): Softmax(dim=1)\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = ANNMultiClassQwerties()\n",
    "epochs = 10000\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "losses = torch.zeros(epochs)\n",
    "accuracies = torch.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # feed forward\n",
    "    yHat = model(data)\n",
    "\n",
    "    # calculate loss\n",
    "    loss = loss_func(yHat, labels)\n",
    "    losses[epoch] = loss\n",
    "    accuracies[epoch] = torch.mean( (yHat.argmax(dim=1) == labels).float() ) * 100\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model performance\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# epoch vs loss\n",
    "axes[0].plot(range(epochs), losses.detach(), marker='o', markerfacecolor='w')\n",
    "axes[0].set_title(f'Epochs vs Loss MIN={losses.min()}')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(range(epochs), accuracies.detach(), marker='o', color='g', markerfacecolor='w')\n",
    "axes[1].set_title(f'Epochs vs Accuracy MAX={accuracies.max()}')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd149c3b",
   "metadata": {},
   "source": [
    "![png](7_ann_53_code_challenge_more_qwerties_files/7_ann_53_code_challenge_more_qwerties_7_0.png)\n",
    "\n",
    "## Question\n",
    "\n",
    "CrossEntropyLoss computes log-softmax internally. Does that mean that the Softmax() layer in the model needs to be there? Does it hurt or help? If you remove that final layer, what would change and what would be the same in the rest of the notebook?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

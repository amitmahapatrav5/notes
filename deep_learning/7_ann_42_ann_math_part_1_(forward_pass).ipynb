{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3685de03",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Section: 7 \\\n",
    "Lecture: 42 \\\n",
    "Title: ANN math part 1 (forward prop) \\\n",
    "TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842118 \\\n",
    "Udemy Reference Link: \\\n",
    "Pre-Requisite:\n",
    "\n",
    "# Basic Mathematics that happens in a single Neuron\n",
    "\n",
    "## Math that happens in every neuron\n",
    "\n",
    "In every Neuron, this is what basically happens \\\n",
    "$Y = \\sigma( bias + \\sum(X^T W) )$ \\\n",
    "But we can write the same thing as $Y = \\sigma( \\sum(X^T W) )$ \\\n",
    "HOW ?\n",
    "\n",
    "$Y = \\sigma( bias + \\sum(X^T W) )$ \\\n",
    "$Y = \\sigma( 1*bias + \\sum(X^T W) )$ \\\n",
    "$Y = \\sigma( 1*W_0 + \\sum(X^T W) )$ \\\n",
    "$Y = \\sigma( 1*W_0 + X_1W_1 + X_2W_2 + ... + X_nW_n )$ \\\n",
    "$Y = \\sigma( X_0*W_0 + X_1W_1 + X_2W_2 + ... + X_nW_n )$ \\\n",
    "$Y = \\sigma( \\sum(X^T W) )$\n",
    "\n",
    "## Why do we need activation function\n",
    "\n",
    "**This is mainly happeing because, if you observe the heatmaps carefully, \\\n",
    "The left figure, where we have not used the activation function, the band value is in range from -10 to 12 \\\n",
    "But in the right figure, where we have used the activation function, the band value is in range from 0 to 1. \\\n",
    "So, in a way, we are kind of transforming the coordinate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def func_linear(x):\n",
    "    return 1 - 2*x[:, 0] + x[:, 1]\n",
    "\n",
    "def func_nonlinear(x):\n",
    "    return torch.sigmoid(1 - 2*x[:, 0] + x[:, 1])\n",
    "\n",
    "start, end = -4, 4\n",
    "nCount = 100\n",
    "\n",
    "# Generate a grid of points\n",
    "x1 = torch.linspace(start, end, nCount)\n",
    "x2 = torch.linspace(start, end, nCount)\n",
    "x1_grid, x2_grid = torch.meshgrid(x1, x2, indexing='xy')\n",
    "x_grid = torch.stack([x1_grid.flatten(), x2_grid.flatten()], dim=1)\n",
    "\n",
    "# Compute the function values\n",
    "z_linear = func_linear(x_grid).reshape(nCount, nCount)\n",
    "z_nonlinear = func_nonlinear(x_grid).reshape(nCount, nCount)\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "x1_np = x1_grid.numpy()\n",
    "x2_np = x2_grid.numpy()\n",
    "z_linear_np = z_linear.numpy()\n",
    "z_nonlinear_np = z_nonlinear.numpy()\n",
    "\n",
    "# Create the heatmaps\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Linear function heatmap\n",
    "contour1 = axs[0].contourf(x1_np, x2_np, z_linear_np, levels=50, cmap='viridis')\n",
    "axs[0].set_title('Linear Function Heatmap')\n",
    "axs[0].set_xlabel('x1')\n",
    "axs[0].set_ylabel('x2')\n",
    "fig.colorbar(contour1, ax=axs[0], label='Function Value')\n",
    "axs[0].axhline(0, color='black', linewidth=0.5, ls='--')\n",
    "axs[0].axvline(0, color='black', linewidth=0.5, ls='--')\n",
    "axs[0].grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Nonlinear function heatmap\n",
    "contour2 = axs[1].contourf(x1_np, x2_np, z_nonlinear_np, levels=50, cmap='viridis')\n",
    "axs[1].set_title('Nonlinear Function Heatmap (with Activation)')\n",
    "axs[1].set_xlabel('x1')\n",
    "axs[1].set_ylabel('x2')\n",
    "fig.colorbar(contour2, ax=axs[1], label='Function Value')\n",
    "axs[1].axhline(0, color='black', linewidth=0.5, ls='--')\n",
    "axs[1].axvline(0, color='black', linewidth=0.5, ls='--')\n",
    "axs[1].grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee7db4",
   "metadata": {},
   "source": [
    "![png](7_ann_42_ann_math_part_1_%28forward_pass%29_files/7_ann_42_ann_math_part_1_%28forward_pass%29_6_0.png)\n",
    "\n",
    "Most Common Activation Functions are\n",
    "\n",
    "1. Sigmoid (Often used in the nodes in the output layer of the model)\n",
    "2. Hyperbolic Tangent (Often used in the nodes in the middle of the model)\n",
    "3. ReLU (Often used in the nodes in the middle of the model)\n",
    "\n",
    "There are lot of other activation functions as well. \\\n",
    "Also, many activation functions are just variants of these 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33607d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

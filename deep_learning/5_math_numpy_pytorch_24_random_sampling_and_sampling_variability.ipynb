{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ff11bf",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Section: 5 \\\n",
    "Lecture: 24 \\\n",
    "Title: Random sampling and sampling variability \\\n",
    "TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27841952 \\\n",
    "Udemy Reference Link: \\\n",
    "Pre-Requisite:\n",
    "\n",
    "# Random Sampling and Sampling Variability\n",
    "\n",
    "Deep Learning requires a lot of data to train the model. But **WHY**?\n",
    "\n",
    "Different samples from the same population can have different values of same measurement. This called **Sampling Variability**.\n",
    "\n",
    "For example, \"What is the average height of an Indian?\" If we collect sample from Assam, and another sample from Punjab, the values will be different.\n",
    "\n",
    "So a Single measurement may be a unreliable estimate of a population parameter.\n",
    "\n",
    "Similarly, not all cat pictures are same. Because not all cats look the same. If that were the case, then we need only 1 picture. But because not all cat looks exactly same, but similar, neural networks need lots of cat images to recognize the pattern of similarity. What we are ultimately doing is, averaging togather many samples. **Averaging togather many samples to approximate the true population mean. This is basically Law of Large Numbers.**\n",
    "\n",
    "The higher the variability there is, the more the number of samples we need. So if we are working with a simple data values, we do not need that many number of samples. But if we are going with a much more complex deep convolutional neural network, we need more number of samples.\n",
    "\n",
    "Below 2 statement need more explanation. \\\n",
    "**Non-random sampling can introduce systematic biases in DL models.** \\\n",
    "**Non-representative sampling causes overfitting and limits generalizability.**\n",
    "\n",
    "## Random Sampling Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d16026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,4,6,5,4,0,-4,5,-2,6,10,-9,1,3,-6]\n",
    "\n",
    "population_mean = np.mean(x) # This will always be constant\n",
    "sample_mean = np.random.choice(x, size=5, replace=False).mean() # This will change\n",
    "\n",
    "population_mean, sample_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43476010",
   "metadata": {},
   "source": [
    "**But if we perform the above example so many times, then we can approximate very close to population mean. This is called Law of Large Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a19ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to learn about histograms to understand the below code\n",
    "\n",
    "nExpers = 10000\n",
    "\n",
    "# run the experiment!\n",
    "sampleMeans = np.zeros(nExpers)\n",
    "for i in range(nExpers):\n",
    "\n",
    "  # step 1: draw a sample\n",
    "  sample = np.random.choice(x,size=15,replace=True)\n",
    "\n",
    "  # step 2: compute its mean\n",
    "  sampleMeans[i] = np.mean(sample)\n",
    "\n",
    "\n",
    "\n",
    "# show the results as a histogram\n",
    "plt.hist(sampleMeans,bins=40,density=True)\n",
    "plt.plot([population_mean,population_mean],[0,.3],'m--')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Sample mean')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

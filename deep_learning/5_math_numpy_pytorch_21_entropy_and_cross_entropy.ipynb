{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9d6615",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Section: 5 \\\n",
    "Lecture: 21 \\\n",
    "Title: Entropy and Cross-Entropy \\\n",
    "TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27841944 \\\n",
    "Udemy Reference Link: \\\n",
    "Pre-Requisite:\n",
    "\n",
    "# Entropy and Cross-Entropy\n",
    "\n",
    "## Formula and Notations\n",
    "\n",
    "### Entropy\n",
    "\n",
    "The entropy $H(X)$ of a discrete random variable $( X $) is defined as:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} P(x_i) \\log P(x_i)\n",
    "$$\n",
    "\n",
    "#### Terms:\n",
    "\n",
    "- $ H(X)$ : Entropy of the random variable $ X $.\n",
    "- $n$ : Number of possible outcomes.\n",
    "- $P(x_i)$ : Probability of outcome $x_i$.\n",
    "- $x_i$ : Possible outcomes of the random variable $X$.\n",
    "\n",
    "### Cross-Entropy\n",
    "\n",
    "The cross-entropy $L $ between the true distribution $( Y $) and the predicted distribution $( P $) is defined as:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{N} y_i \\log(p_i)\n",
    "$$\n",
    "\n",
    "#### Terms:\n",
    "\n",
    "- $ L $: Cross-entropy loss.\n",
    "- $ N $: Number of classes or outcomes.\n",
    "- $ y_i $: True label (1 if the class is the correct label, 0 otherwise).\n",
    "- $ p_i $: Predicted probability of class $ i $.\n",
    "\n",
    "**In both the formula, -ve sign is there, because $p_i$ is between [0,1] and log of something between 0 and 1 is -ve.**\n",
    "\n",
    "High entropy means that the dataset has a log of variability. Low entropy means that most of the values of the dataset repeat(and therefore are redundant).\n",
    "\n",
    "**How does entropy differs from variance?**\n",
    "Both Entropy and Variance is a measure of distribution. But, I did not understand this\n",
    "\n",
    "## Binary Cross Entropy using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy Calculation\n",
    "x = [.20,.70, 0.10] # This is 1 event/1 experiment/1 set of datapoint\n",
    "print(np.sum(x)) # Hence almost 1\n",
    "\n",
    "H = 0\n",
    "for p in x:\n",
    "  H -= p*np.log(p)\n",
    "\n",
    "print('Entropy: ' + str(H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moreover If there are only 2 possibilities, then its called Binary Entropy\n",
    "x = [0.75, 0.25] # 2 possibilities in 1 event/ 1 experiment/ 1 set of datapoint\n",
    "H = -( x[0]*np.log(x[0]) + x[1]*np.log(x[1]) )\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074849c4",
   "metadata": {},
   "source": [
    "In deep learning, we use cross-entropy as a loss function because it measures the difference between two probability distributions: the true probabilities derived from the training data and the predicted probabilities generated by the model. Since these probabilities are typically different, cross-entropy is employed to quantify this discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]\n",
    "model_output = [0.75, 0.25]\n",
    "\n",
    "H = -( labels[0]*np.log(model_output[0]) + labels[1]*np.log(model_output[1]) )\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c7dee",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54733f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1, 0], dtype=torch.float32)\n",
    "model_output = torch.tensor([0.75, 0.25], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9477ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating manually\n",
    "H = -( labels[0]*torch.log(model_output[0]) + labels[1]*torch.log(model_output[1]) )\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating using inbuilt torch function\n",
    "binary_cross_entropy = torch.nn.BCELoss()\n",
    "binary_cross_entropy(model_output, labels) # Binary cross entropy of torch is a bit sensitive to what is the order in which we pass the parameter."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

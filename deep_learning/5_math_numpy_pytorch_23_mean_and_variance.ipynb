{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39482274",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Section: 5 \\\n",
    "Lecture: 23 \\\n",
    "Title: Mean and variance \\\n",
    "TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27841948 \\\n",
    "Udemy Reference Link: \\\n",
    "Pre-Requisite:\n",
    "\n",
    "# Mean and Variance\n",
    "\n",
    "## Mean/Average\n",
    "\n",
    "Mean and Average are the same things.\n",
    "\n",
    "We calculate mean, median, and mode of a distribution. All these three are different, though, and each one has its own use case and is useful in specific distributions to display meaning.\n",
    "\n",
    "Mean/Average basically tells us a value at the center of a distribution.\n",
    "\n",
    "**Formula:**\n",
    "$\n",
    "\\text{Mean} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73dd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cd749",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "unimodal_data_1 = np.random.normal(loc=0, scale=1, size=100000)  # First mode\n",
    "unimodal_data_2 = np.random.normal(loc=5, scale=1, size=100000)  # Second mode\n",
    "bimodal_data = np.hstack([unimodal_data_1, unimodal_data_2])\n",
    "left_skewed_data = np.random.exponential(scale=0.125, size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "sns.kdeplot(unimodal_data_1, color='b', alpha=0.5)\n",
    "plt.axvline(np.mean(unimodal_data_1), color='k', linestyle='--', label='Mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d7ac51",
   "metadata": {},
   "source": [
    "**But mean does not always work in all kinds of distributions. For example, in bimodal distributions or non-Gaussian distributions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a38750",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "sns.kdeplot(bimodal_data, color='b', alpha=0.5)\n",
    "plt.axvline(np.mean(bimodal_data), color='k', linestyle='--', label='Mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "sns.kdeplot(left_skewed_data, color='b', alpha=0.5)\n",
    "plt.axvline(np.mean(left_skewed_data), color='k', linestyle='--', label='Mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139c8fa",
   "metadata": {},
   "source": [
    "## Variance and Standard Deviation\n",
    "\n",
    "Variance/Standard Deviation is the measure of how much **data points differ from the mean**. Standard Deviation is basically the square root of Variance. Nothing else.\n",
    "\n",
    "Larger variance means that if we plot the points on a number line, the numbers will be largely distributed. However, smaller variance means that if we plot the points on a number line, the numbers will be close to each other comparatively.\n",
    "\n",
    "**Variance Formula:**\n",
    "$\n",
    "\\text{Variance} = \\frac{\\sum_{i=1}^{n} (x_i - \\text{Mean})^2}{n}\n",
    "$\n",
    "\n",
    "**Standard Deviation Formula:**\n",
    "$\n",
    "\\text{Standard Deviation} = \\sqrt{\\text{Variance}}\n",
    "$\n",
    "\n",
    "**Why do we subtract the mean from each number?**\n",
    "\n",
    "Because we want the same variance for [1, 2, 3, 3, 2, 1] and [101, 102, 103, 103, 102, 101] (just added 100 to each number).\n",
    "\n",
    "**Why do we square?**\n",
    "\n",
    "Squaring adds a nice property to numbers, like being able to differentiate, it is continuous, etc. If we do not square, then the distribution variance will always result in 0.\n",
    "\n",
    "**Why not take MOD to solve the 0 Variance problem?**\n",
    "\n",
    "There is a formula called Mean Absolute Difference (MAD). But this is less common. This concept is used in L1 regularization. However, the squared variation is used in L2 Regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862dc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "sns.kdeplot(unimodal_data_1, color='b', alpha=0.5)\n",
    "plt.axvline(np.mean(unimodal_data_1), color='k', linestyle='--', label='Mean')\n",
    "\n",
    "plt.axvline(np.mean(unimodal_data_1), color='k', linestyle='--', label='Mean')\n",
    "plt.axvline(np.mean(unimodal_data_1) + np.std(unimodal_data_1), color='r', linestyle='--', label='Mean + 1 Std Dev')\n",
    "plt.axvline(np.mean(unimodal_data_1) - np.std(unimodal_data_1), color='g', linestyle='--', label='Mean - 1 Std Dev')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c068e",
   "metadata": {},
   "source": [
    "## Example in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,4,6,5,4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(x), np.sum(x)/len(x) # both are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e026fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(x), np.sum((x - np.mean(x))**2) / ( len(x)-1 ) # both are different => because of degree of freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(x, ddof=1), np.sum((x - np.mean(x))**2) / ( len(x)-1 ) # now both are same. BUT this does not matter in huge data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a4df8",
   "metadata": {},
   "source": [
    "## Example in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,4,6,5,4,0], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a47822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in torch, ddof param is same as correction and equal to 1 by default\n",
    "torch.mean(x), torch.var(x), torch.sum(( x - torch.mean(x) )**2) / (len(x)-1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

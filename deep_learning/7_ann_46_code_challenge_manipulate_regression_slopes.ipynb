{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b237ca9",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Section: 7 \\\n",
    "Lecture: 46 \\\n",
    "Title: CodeChallenge: manipulate regression slopes \\\n",
    "TCS Udemy Reference Link: https://tcsglobal.udemy.com/course/deeplearning_x/learn/lecture/27842128 \\\n",
    "Udemy Reference Link: \\\n",
    "Pre-Requisite:\n",
    "\n",
    "# Parametric Experiment - Slope vs Loss and Slope vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b27a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANNRegression, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(1, 1),\n",
    "                nn.ReLU(),\n",
    "            nn.Linear(1, 1)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.stack(X)\n",
    "\n",
    "ANNRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f79cb6",
   "metadata": {},
   "source": [
    "    ANNRegression(\n",
    "      (stack): Sequential(\n",
    "        (0): Linear(in_features=1, out_features=1, bias=True)\n",
    "        (1): ReLU()\n",
    "        (2): Linear(in_features=1, out_features=1, bias=True)\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7299cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(X, y):\n",
    "\n",
    "    # built the model\n",
    "    model = ANNRegression()\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train the model\n",
    "    epochs = 500\n",
    "    losses = torch.zeros(epochs)\n",
    "    for epoch in range(epochs):\n",
    "        # feed forward\n",
    "        yHat = model(X)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(yHat, y)\n",
    "        losses[epoch] = loss\n",
    "\n",
    "        # back propagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    prediction = model(X)\n",
    "    return prediction, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd108402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(m):\n",
    "    N = 50\n",
    "\n",
    "    X = torch.randn(N, 1)\n",
    "    y = m*X + torch.randn(N, 1)/2\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81051e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test everything once\n",
    "X, y = generate_data(m=1)\n",
    "prediction, losses = build_and_train(X, y)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Reality vs Model Prediction\n",
    "axes[0].scatter(X, y, color='b', marker='+', label='Reality')\n",
    "axes[0].scatter(X, prediction.detach(), marker='*', color='g', label='Model Prediction')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('Y')\n",
    "axes[0].set_title('Reality vs Model Prediction')\n",
    "axes[0].legend()\n",
    "\n",
    "# epoch vs loss\n",
    "axes[1].plot(range(len(losses)), losses.detach(), color='m', marker='o')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Epoch vs Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7541230",
   "metadata": {},
   "source": [
    "![png](7_ann_46_code_challenge_manipulate_regression_slopes_files/7_ann_46_code_challenge_manipulate_regression_slopes_6_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Experiment\n",
    "slopes = torch.linspace(-2, 2, 21)\n",
    "exps = 50\n",
    "\n",
    "result = torch.zeros(len(slopes), exps, 2)\n",
    "\n",
    "for slp in range(len(slopes)):\n",
    "    for exp in range(exps):\n",
    "        X, y = generate_data(slopes[slp])\n",
    "        prediction, losses = build_and_train(X, y)\n",
    "\n",
    "        # store the performance and final loss\n",
    "        result[slp, exp, 0] = torch.corrcoef(torch.vstack((prediction.T, y.T)))[0, 1]\n",
    "        result[slp, exp, 1] = losses[-1]\n",
    "result[torch.isnan(result)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ecfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].plot(slopes.detach(), torch.mean(result[:, :, 0], dim=1).detach(), marker='o', color='b')\n",
    "axes[0].set_title('Slope Vs Performance')\n",
    "axes[0].set_xlabel('Slope')\n",
    "axes[0].set_ylabel('co-relation between reality and prediction')\n",
    "\n",
    "axes[1].plot(slopes.detach(), torch.mean(result[:, :, 1], dim=1).detach(), marker='s', color='m')\n",
    "axes[1].set_title('Slope Vs Loss')\n",
    "axes[1].set_xlabel('Slope')\n",
    "axes[1].set_ylabel('Last Training Epoch Model Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2c69e",
   "metadata": {},
   "source": [
    "![png](7_ann_46_code_challenge_manipulate_regression_slopes_files/7_ann_46_code_challenge_manipulate_regression_slopes_8_0.png)\n",
    "\n",
    "### Why Performance is very low in case of slope=0?\n",
    "\n",
    "When slope=0, the model does not get more relation between x and y. However, in case of other slopes, the model could learn the relation between the x and y. You can see the same from below diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "X, y = generate_data(m=0)\n",
    "axes[0].scatter(X, y)\n",
    "\n",
    "X, y = generate_data(m=2)\n",
    "axes[1].scatter(X, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ea1ea",
   "metadata": {},
   "source": [
    "![png](7_ann_46_code_challenge_manipulate_regression_slopes_files/7_ann_46_code_challenge_manipulate_regression_slopes_10_0.png)\n",
    "\n",
    "### Why Loss is very low in case of slope=0 and high in other cases?\n",
    "\n",
    "This is because, in case of higher slopes, the variance of y is higher, but in case where slope is 0, the variance is lower. Normalization of data can solve this issue. In the below diagram, when slope=0, the variance is from -1 to 1, but when slope=2, variance is from -4 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182541e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "X, y = generate_data(m=0)\n",
    "axes[0].scatter(X, y)\n",
    "\n",
    "X, y = generate_data(m=2)\n",
    "axes[1].scatter(X, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0565be5",
   "metadata": {},
   "source": [
    "![png](7_ann_46_code_challenge_manipulate_regression_slopes_files/7_ann_46_code_challenge_manipulate_regression_slopes_12_0.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
